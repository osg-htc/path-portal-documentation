{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"htc_workloads/automated_workflows/dagman-workflows/","text":"Intermediate DAGMan: Uses and Features \u00b6 This tutorial helps you explore HTCondor's DAGMan its many features. You can download the tutorial materials with the following command: $ git clone https://github.com/OSGConnect/tutorial-dagman-intermediate Now move into the new directory to see the contents of the tutorial: $ cd tutorial-dagman-intermediate At the top level is a worked example of a \"Diamond DAG\" that summarizes the basic components of a creating, submitting, and managing DAGMan workflows. In the lower level additional_examples directory are more worked examples with their own README s highlighting specific features that can be used with DAGMan. Brief descriptions of these examples are provided in the Additional Examples section at the end of this tutorial. Before working on this tutorial, we recommend that you read through our other DAGMan guides: Overview: Submit Workflows with HTCondor's DAGMan Simple Example of a DAGMan Workflow The definitive guide to DAGMan is HTCondor's DAGMan Documentation . Types of DAGs \u00b6 While any workflow that satisfies the definition of a \"Directed Acyclic Graph\" (DAG) can be executed using DAGMan, there are certain types that are the most commonly used: Sequential DAG : all the nodes are connected in a sequence of one after the other, with no branching or splitting. This is good for conducting increasingly refined analyses of a dataset or initial result, or chaining together a long-running calculation. The simplest example of this type is used in the guide Simple Example of a DAGMan Workflow . Split and recombine DAG : the first node is connected to many nodes of the same layer (split) which then all connect back to the final node (recombine). Here, you can set up the shared environment in the first node and use it to parallelize the work into many individual jobs, then finally combine/analyze the results in the final node. The simplest example of this type is the \"Diamond DAG\" - the subject of this tutorial. Collection DAG : no node is connected to any other node. This is good for the situation where you need to run a bunch of otherwise unrelated jobs, perhaps ones that are competing for a limited resource. The simplest example of this type is a DAG consisting of a single node. These types are by no means \"official\", nor are they the only types of structure that a DAG can take. Rather, they serve as starting points from which you can build your own DAG workflow, which will likely consist of some combination of the above elements. The Diamond DAG \u00b6 As mentioned above, the \"Diamond DAG\" is the simplest example of a \"split and recombine\" DAG. In this case, the first node TOP is connected to two nodes LEFT and RIGHT (the \"split\"), which are then connected to the final node BOTTOM (the \"recombine\"). To describe the flow of the DAG and the parts needed to execute it, DAGMan uses a custom description language in an input file, typically named <DAG Name>.dag . The two most important commands in the DAG description language are: JOB <NodeName> <NodeSubmitFile> - Describes a node and the submit file it will use to run the node. PARENT <NodeName1> CHILD <NodeName2> - Describes the edge starting from <NodeName1> and pointing to <NodeName2> . These commands have been used to construct the Diamond DAG and are saved in the file diamond.dag . To view the contents of diamond.dag , run $ cat diamond.dag Before you continue, we recommend that you closely examine the contents of diamond.dag and identify its components. Furthermore, try to identify the submit file for each node, and use that submit file to determine the nature of the HTCondor job that will be submitted for each node. Submitting a DAG \u00b6 To submit a DAGMan workflow to HTCondor, you can use one of the following commands: $ condor_submit_dag diamond.dag or $ htcondor dag submit diamond.dag What Happens? \u00b6 When a DAG is submitted to HTCondor a special job is created to run DAGMan on behalf of you the user. This job runs the provided HTCSS DAGMan executable in the AP job queue. This is an actual job that can be queried and acted upon. You may also notice that lots of files are created. These files are all part of DAGMan and have various purposes. In general, the files that should always exist are as follows: DAGMan job proper files <DAG Name>.condor.sub - Submit file for the DAGMan job proper <DAG Name>.dagman.log - Job event log file for the DAGMan job proper <DAG Name>.lib.err - Standard error stream file for the DAGMan job proper <DAG Name>.lib.out - Standard output stream file for the DAGMan job proper Informational DAGMan files <DAG Name>.dagman.out - General DAGMan process logging file <DAG Name>.nodes.log - Collective job event log file for all managed jobs (Heart of DAGMan) <DAG Name>.metrics - JSON formatted information about the DAG Of these files, the two most important are the <DAG Name>.dagman.out and <DAG Name>.nodes.log . The .dagman.out file contains the entire history and status of DAGMan's execution of your workflow. The .nodes.log file on the other hand is the accumulated log entries for every HTCondor job that DAGMan submitted, and DAGMan monitors the contents of this file to generate the contents of the .dagman.out file. Note: these are not all the files that DAGMan can produce. Depending on the options and features you employ in your DAG input file, more files with different purposes can be created. Monitoring DAGMan \u00b6 The DAGMan job and the jobs in the DAG workflow can be found in the AP job queue and so the normal methods of job monitoring work. That also means that you can interact with these jobs, though in a more limited fashion than a regular job (see Running and Managing DAGMan for more details). A plain condor_q command will show a condensed batch view of the jobs submitted, running, and managed by the DAGMan job proper. For more information about jobs running under DAGMan, use the -nobatch and -dag flags: # Basic job query (Batched/Condensed) $ condor_q # Non-Batched query $ condor_q -nobatch # Increased information $ condor_q -nobatch -dag You can also watch the progress of the DAG and the jobs running under it by running: $ condor_watch_q Note that condor_watch_q works by monitoring the log files of jobs that are in the queue, but only at the time of its execution. Additional jobs submitted by DAGMan while condor_watch_q is running will not appear in condor_watch_q . To see additional jobs as they are submitted, wait for DAGMan to create the .nodes.log file, then run $ condor_watch_q -files *.log For more detail about the status and progress of your DAG workflow, you can use the noun-verb command: $ htcondor dag status DAGManJobID where DAGManJobID is the ID for the DAGMan job proper. Note that the information in the output of this command does not update frequently, and so it is not suited for short-lived DAG workflows such as the current example. When your DAG workflow has completed, the DAGMan job proper will disappear from the queue. If the DAG workflow completed successfully, then the .dag.dagman.out file should have a message that All jobs Completed! , though it may be difficult to find manually (try using grep \"All jobs Completed!\" *.dag.dagman.out instead). If the DAG workflow was aborted due to an error, then the .dag.dagman.out file should have the message Aborting DAG... . Assuming that the DAGMan job proper did not crash, then regardless the final line of the .dag.dagman.out file should contain (condor_DAGMAN) pid ####### EXITING WITH STATUS # , where the number after STATUS is the exit code (0 if success, not 0 if failure). How DAGMan Handles Relative Paths \u00b6 By default, the directory that DAGMan submits all jobs from is the same directory you are in when you run condor_submit_dag . This directory (let's call it the submit directory) is the starting directory for any relative path in the .dag input file or in the node .sub files that DAGMan submits . This can be observed by inspecting the sleep.sub submit file in the SleepJob sub-directory and by inspecting the diamond.dag input file. In the diamond.dag file, the jobs are declared using a relative path. For example: JOB TOP ./SleepJob/sleep.sub This tells DAGMan that the submit file for the JOB TOP is sleep.sub , located in the SleepJob in the submit directory ( . ). Similarly, the submit file sleep.sub uses paths relative to the submit directory for defining the save locations for the .log , .out , and .err files, i.e., log = ./SleepJob/$(JOB).log This behavior is consistent with submission of regular (non-DAGMan) jobs, e.g. condor_submit SleepJob/sleep.sub . Contrary to the above behavior, the .dag.* log/output files generated by the DAGMan job proper will always be in the same directory as the .dag input file. This is just the default behavior, and there are ways to make the location of job submission/management more obvious. See the HTCondor documentation for more details: File Paths in DAGs . Additional Examples \u00b6 Additional examples that cover various topics related to DAGMan are provided in the folder additional_examples with corresponding READMEs. The following order of the examples is recommended: RescueDag - Example for DAGs that don't exit successfully PreScript - Example using a pre-script for a node PostScript - Example using a post-script for a node Retry - Example for retrying a failed node VARS - Example of reusing a single submit file for multiple nodes with differing variables SubDAG (advanced) - Example using a subDAG Splice (advanced) - Example of using DAG splices","title":"Submit Workflows with HTCondor's DAGMan "},{"location":"htc_workloads/automated_workflows/dagman-workflows/#intermediate-dagman-uses-and-features","text":"This tutorial helps you explore HTCondor's DAGMan its many features. You can download the tutorial materials with the following command: $ git clone https://github.com/OSGConnect/tutorial-dagman-intermediate Now move into the new directory to see the contents of the tutorial: $ cd tutorial-dagman-intermediate At the top level is a worked example of a \"Diamond DAG\" that summarizes the basic components of a creating, submitting, and managing DAGMan workflows. In the lower level additional_examples directory are more worked examples with their own README s highlighting specific features that can be used with DAGMan. Brief descriptions of these examples are provided in the Additional Examples section at the end of this tutorial. Before working on this tutorial, we recommend that you read through our other DAGMan guides: Overview: Submit Workflows with HTCondor's DAGMan Simple Example of a DAGMan Workflow The definitive guide to DAGMan is HTCondor's DAGMan Documentation .","title":"Intermediate DAGMan: Uses and Features"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#types-of-dags","text":"While any workflow that satisfies the definition of a \"Directed Acyclic Graph\" (DAG) can be executed using DAGMan, there are certain types that are the most commonly used: Sequential DAG : all the nodes are connected in a sequence of one after the other, with no branching or splitting. This is good for conducting increasingly refined analyses of a dataset or initial result, or chaining together a long-running calculation. The simplest example of this type is used in the guide Simple Example of a DAGMan Workflow . Split and recombine DAG : the first node is connected to many nodes of the same layer (split) which then all connect back to the final node (recombine). Here, you can set up the shared environment in the first node and use it to parallelize the work into many individual jobs, then finally combine/analyze the results in the final node. The simplest example of this type is the \"Diamond DAG\" - the subject of this tutorial. Collection DAG : no node is connected to any other node. This is good for the situation where you need to run a bunch of otherwise unrelated jobs, perhaps ones that are competing for a limited resource. The simplest example of this type is a DAG consisting of a single node. These types are by no means \"official\", nor are they the only types of structure that a DAG can take. Rather, they serve as starting points from which you can build your own DAG workflow, which will likely consist of some combination of the above elements.","title":"Types of DAGs"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#the-diamond-dag","text":"As mentioned above, the \"Diamond DAG\" is the simplest example of a \"split and recombine\" DAG. In this case, the first node TOP is connected to two nodes LEFT and RIGHT (the \"split\"), which are then connected to the final node BOTTOM (the \"recombine\"). To describe the flow of the DAG and the parts needed to execute it, DAGMan uses a custom description language in an input file, typically named <DAG Name>.dag . The two most important commands in the DAG description language are: JOB <NodeName> <NodeSubmitFile> - Describes a node and the submit file it will use to run the node. PARENT <NodeName1> CHILD <NodeName2> - Describes the edge starting from <NodeName1> and pointing to <NodeName2> . These commands have been used to construct the Diamond DAG and are saved in the file diamond.dag . To view the contents of diamond.dag , run $ cat diamond.dag Before you continue, we recommend that you closely examine the contents of diamond.dag and identify its components. Furthermore, try to identify the submit file for each node, and use that submit file to determine the nature of the HTCondor job that will be submitted for each node.","title":"The Diamond DAG"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#submitting-a-dag","text":"To submit a DAGMan workflow to HTCondor, you can use one of the following commands: $ condor_submit_dag diamond.dag or $ htcondor dag submit diamond.dag","title":"Submitting a DAG"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#what-happens","text":"When a DAG is submitted to HTCondor a special job is created to run DAGMan on behalf of you the user. This job runs the provided HTCSS DAGMan executable in the AP job queue. This is an actual job that can be queried and acted upon. You may also notice that lots of files are created. These files are all part of DAGMan and have various purposes. In general, the files that should always exist are as follows: DAGMan job proper files <DAG Name>.condor.sub - Submit file for the DAGMan job proper <DAG Name>.dagman.log - Job event log file for the DAGMan job proper <DAG Name>.lib.err - Standard error stream file for the DAGMan job proper <DAG Name>.lib.out - Standard output stream file for the DAGMan job proper Informational DAGMan files <DAG Name>.dagman.out - General DAGMan process logging file <DAG Name>.nodes.log - Collective job event log file for all managed jobs (Heart of DAGMan) <DAG Name>.metrics - JSON formatted information about the DAG Of these files, the two most important are the <DAG Name>.dagman.out and <DAG Name>.nodes.log . The .dagman.out file contains the entire history and status of DAGMan's execution of your workflow. The .nodes.log file on the other hand is the accumulated log entries for every HTCondor job that DAGMan submitted, and DAGMan monitors the contents of this file to generate the contents of the .dagman.out file. Note: these are not all the files that DAGMan can produce. Depending on the options and features you employ in your DAG input file, more files with different purposes can be created.","title":"What Happens?"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#monitoring-dagman","text":"The DAGMan job and the jobs in the DAG workflow can be found in the AP job queue and so the normal methods of job monitoring work. That also means that you can interact with these jobs, though in a more limited fashion than a regular job (see Running and Managing DAGMan for more details). A plain condor_q command will show a condensed batch view of the jobs submitted, running, and managed by the DAGMan job proper. For more information about jobs running under DAGMan, use the -nobatch and -dag flags: # Basic job query (Batched/Condensed) $ condor_q # Non-Batched query $ condor_q -nobatch # Increased information $ condor_q -nobatch -dag You can also watch the progress of the DAG and the jobs running under it by running: $ condor_watch_q Note that condor_watch_q works by monitoring the log files of jobs that are in the queue, but only at the time of its execution. Additional jobs submitted by DAGMan while condor_watch_q is running will not appear in condor_watch_q . To see additional jobs as they are submitted, wait for DAGMan to create the .nodes.log file, then run $ condor_watch_q -files *.log For more detail about the status and progress of your DAG workflow, you can use the noun-verb command: $ htcondor dag status DAGManJobID where DAGManJobID is the ID for the DAGMan job proper. Note that the information in the output of this command does not update frequently, and so it is not suited for short-lived DAG workflows such as the current example. When your DAG workflow has completed, the DAGMan job proper will disappear from the queue. If the DAG workflow completed successfully, then the .dag.dagman.out file should have a message that All jobs Completed! , though it may be difficult to find manually (try using grep \"All jobs Completed!\" *.dag.dagman.out instead). If the DAG workflow was aborted due to an error, then the .dag.dagman.out file should have the message Aborting DAG... . Assuming that the DAGMan job proper did not crash, then regardless the final line of the .dag.dagman.out file should contain (condor_DAGMAN) pid ####### EXITING WITH STATUS # , where the number after STATUS is the exit code (0 if success, not 0 if failure).","title":"Monitoring DAGMan"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#how-dagman-handles-relative-paths","text":"By default, the directory that DAGMan submits all jobs from is the same directory you are in when you run condor_submit_dag . This directory (let's call it the submit directory) is the starting directory for any relative path in the .dag input file or in the node .sub files that DAGMan submits . This can be observed by inspecting the sleep.sub submit file in the SleepJob sub-directory and by inspecting the diamond.dag input file. In the diamond.dag file, the jobs are declared using a relative path. For example: JOB TOP ./SleepJob/sleep.sub This tells DAGMan that the submit file for the JOB TOP is sleep.sub , located in the SleepJob in the submit directory ( . ). Similarly, the submit file sleep.sub uses paths relative to the submit directory for defining the save locations for the .log , .out , and .err files, i.e., log = ./SleepJob/$(JOB).log This behavior is consistent with submission of regular (non-DAGMan) jobs, e.g. condor_submit SleepJob/sleep.sub . Contrary to the above behavior, the .dag.* log/output files generated by the DAGMan job proper will always be in the same directory as the .dag input file. This is just the default behavior, and there are ways to make the location of job submission/management more obvious. See the HTCondor documentation for more details: File Paths in DAGs .","title":"How DAGMan Handles Relative Paths"},{"location":"htc_workloads/automated_workflows/dagman-workflows/#additional-examples","text":"Additional examples that cover various topics related to DAGMan are provided in the folder additional_examples with corresponding READMEs. The following order of the examples is recommended: RescueDag - Example for DAGs that don't exit successfully PreScript - Example using a pre-script for a node PostScript - Example using a post-script for a node Retry - Example for retrying a failed node VARS - Example of reusing a single submit file for multiple nodes with differing variables SubDAG (advanced) - Example using a subDAG Splice (advanced) - Example of using DAG splices","title":"Additional Examples"},{"location":"htc_workloads/containers/available-containers-list/","text":"Existing OSPool-Supported Containers \u00b6 This is list of commonly used containers in the Open Science Pool and PATh Facility. These can be used directly in your jobs or as base images if you want to define your own. Base \u00b6 Debian 12 (htc/debian:12) Debian 12 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__debian__12.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/debian:12 Project Website Container Definition EL 7 (htc/centos:7) Enterprise Linux (CentOS) 7 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__centos__7.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/centos:7 Project Website Container Definition Rocky 8 (htc/rocky:8) Rocky Linux 8 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:8 Project Website Container Definition Rocky 8 / CUDA 11.0.3 (htc/rocky:8-cuda-11.0.3) Rocky Linux 8 / CUDA 11.0.3 image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__8-cuda-11.0.3.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:8-cuda-11.0.3 Project Website Container Definition Rocky 9 (htc/rocky:9) Rocky Linux 9 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:9 Project Website Container Definition Rocky 9 / CUDA 2.6.0 (htc/rocky:9-cuda-12.6.0) Rocky Linux 9 / CUDA 12.6.0 image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9-cuda-12.6.0.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:9-cuda-12.6.0 Project Website Container Definition Ubuntu 20.04 (htc/ubuntu:20.04) Ubuntu 20.04 (Focal) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__20.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:20.04 Project Website Container Definition Ubuntu 22.04 (htc/ubuntu:22.04) Ubuntu 22.04 (Jammy Jellyfish) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__22.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:22.04 Project Website Container Definition Ubuntu 24.04 (htc/ubuntu:24.04) Ubuntu 24.04 (Nobel Numbat) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__24.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:24.04 Project Website Container Definition AI \u00b6 Tensorflow 2.15 (htc/tensorflow:2.15) Tensorflow image from the Tensorflow project, with OSG additions OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__tensorflow__2.15.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/tensorflow:2.15 Project Website Container Definition scikit-learn:1.3.2 (htc/scikit-learn:1.3) scikit-learn, configured for execution on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__scikit-learn__1.3.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/scikit-learn:1.3 Project Website Container Definition Languages \u00b6 Julia (opensciencegrid/osgvo-julia) Ubuntu based image with Julia OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.0.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.5.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.7.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.0.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.5.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.7.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:latest Project Website Container Definition Julia (m8zeng/julia-packages) Ubuntu based image with Julia OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/m8zeng__julia-packages__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/m8zeng/julia-packages:latest Project Website Container Definition Matlab Runtime (opensciencegrid/osgvo-matlab-runtime) This is the Matlab runtime component you can use to execute compiled Matlab codes OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2018b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2019a.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2019b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2020a.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2020b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2021b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2022b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2023a.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2021b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2022b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2023a Project Website Container Definition Matlab Runtime (htc/matlab-runtime:R2023a) This is the Matlab runtime component you can use to execute compiled Matlab codes OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__matlab-runtime__R2023a.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/matlab-runtime:R2023a Project Website Container Definition R (opensciencegrid/osgvo-r) Example for building R images OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__3.5.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__4.0.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:4.0.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:latest Project Website Container Definition R (clkwisconsin/spacetimer) Example for building R images OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/clkwisconsin__spacetimer__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/clkwisconsin/spacetimer:latest Project Website Container Definition Project \u00b6 XENONnT (opensciencegrid/osgvo-xenon) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.11.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.11.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.12.21.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.12.23.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.11.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.04.18.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.05.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.06.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.07.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.08.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.08.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.05.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.05.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.07.27.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.09.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__add_latex.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__gpu.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__latex_test3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__py38.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__stable.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__straxen_0-13-1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__straxen_v100.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__switch_deployhq_user.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__upgrade-boost.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.12.21 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.12.23 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.11 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.04.18 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.05.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.06.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.07.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.08.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.08.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.05.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.05.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.07.27 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.09.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.11.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:add_latex /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:gpu /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:latex_test3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:py38 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:stable /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:straxen_0-13-1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:straxen_v100 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:switch_deployhq_user /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:upgrade-boost Project Website Container Definition XENONnT (xenonnt/base-environment) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.11.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.11.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.21.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.23.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.24.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.11.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.04.18.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.05.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.06.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.07.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.08.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.08.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.05.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.05.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.07.27.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.09.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__add_latex.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__gpu.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__latex_test3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__py38.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__stable.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__straxen_v100.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__switch_deployhq_user.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__testing.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__upgrade-boost.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.21 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.23 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.24 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.04 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.11 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.04.18 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.05.04 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.06.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.07.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.08.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.08.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.6 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.05.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.05.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.6 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.07.27 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.09.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.11.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:add_latex /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:gpu /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:latex_test3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:py38 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:stable /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:straxen_v100 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:switch_deployhq_user /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:testing /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:upgrade-boost Project Website Container Definition XENONnT (xenonnt/osg_dev) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__osg_dev__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/xenonnt/osg_dev:latest Project Website Container Definition Tools \u00b6 DeepLabCut 3.0.0rc3 (htc/deeplabcut:3.0.0rc4) A software package for animal pose estimation OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__deeplabcut__3.0.0rc4.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/deeplabcut:3.0.0rc4 Project Website Container Definition FreeSurfer (opensciencegrid/osgvo-freesurfer) A software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__6.0.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__6.0.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__7.0.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__7.1.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.1.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest Project Website Container Definition GROMACS (opensciencegrid/osgvo-gromacs) A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__2018.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__2020.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:2018.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:2020.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:latest Project Website Container Definition GROMACS GPU (opensciencegrid/osgvo-gromacs-gpu) A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a GPU enabled version. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs-gpu__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs-gpu:latest Project Website Container Definition Gromacs 2023.4 (htc/gromacs:2023.4) Gromacs 2023.4 for use on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__gromacs__2023.4.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/gromacs:2023.4 Project Website Container Definition Gromacs 2024.2 (htc/gromacs:2024.2) Gromacs 2024.2 for use on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__gromacs__2024.2.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/gromacs:2024.2 Project Website Container Definition Minimal (htc/minimal:0) Minimal image - used for testing OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__minimal__0.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/minimal:0 Project Website Container Definition PyTorch 2.3.1 (htc/pytorch:2.3.1-cuda11.8) A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__pytorch__2.3.1-cuda11.8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/pytorch:2.3.1-cuda11.8 Project Website Container Definition Quantum Espresso (opensciencegrid/osgvo-quantum-espresso) A suite for first-principles electronic-structure calculations and materials modeling OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-quantum-espresso__6.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-quantum-espresso__6.8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.8 Project Website Container Definition RASPA2 (opensciencegrid/osgvo-raspa2) General purpose classical simulation package. It can be used for the simulation of molecules in gases, fluids, zeolites, aluminosilicates, metal-organic frameworks, carbon nanotubes and external fields. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-raspa2__2.0.41.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-raspa2:2.0.41 Project Website Container Definition TensorFlow (opensciencegrid/tensorflow) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow__2.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest Project Website Container Definition TensorFlow (rynge/tensorflow-cowsay) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/rynge__tensorflow-cowsay__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/rynge/tensorflow-cowsay:latest Project Website Container Definition TensorFlow (jiahe58/tensorflow) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/jiahe58__tensorflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/jiahe58/tensorflow:latest Project Website Container Definition TensorFlow GPU (opensciencegrid/tensorflow-gpu) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__2.2-cuda-10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__2.3-cuda-10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest Project Website Container Definition TensorFlow GPU (efajardo/astroflow) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/efajardo__astroflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/efajardo/astroflow:latest Project Website Container Definition TensorFlow GPU (ssrujanaa/catsanddogs) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/ssrujanaa__catsanddogs__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/ssrujanaa/catsanddogs:latest Project Website Container Definition TensorFlow GPU (weiphy/skopt) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/weiphy__skopt__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/weiphy/skopt:latest Project Website Container Definition","title":"Containers - Predefined List"},{"location":"htc_workloads/containers/available-containers-list/#existing-ospool-supported-containers","text":"This is list of commonly used containers in the Open Science Pool and PATh Facility. These can be used directly in your jobs or as base images if you want to define your own.","title":"Existing OSPool-Supported Containers"},{"location":"htc_workloads/containers/available-containers-list/#base","text":"Debian 12 (htc/debian:12) Debian 12 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__debian__12.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/debian:12 Project Website Container Definition EL 7 (htc/centos:7) Enterprise Linux (CentOS) 7 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__centos__7.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/centos:7 Project Website Container Definition Rocky 8 (htc/rocky:8) Rocky Linux 8 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:8 Project Website Container Definition Rocky 8 / CUDA 11.0.3 (htc/rocky:8-cuda-11.0.3) Rocky Linux 8 / CUDA 11.0.3 image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__8-cuda-11.0.3.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:8-cuda-11.0.3 Project Website Container Definition Rocky 9 (htc/rocky:9) Rocky Linux 9 base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:9 Project Website Container Definition Rocky 9 / CUDA 2.6.0 (htc/rocky:9-cuda-12.6.0) Rocky Linux 9 / CUDA 12.6.0 image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9-cuda-12.6.0.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/rocky:9-cuda-12.6.0 Project Website Container Definition Ubuntu 20.04 (htc/ubuntu:20.04) Ubuntu 20.04 (Focal) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__20.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:20.04 Project Website Container Definition Ubuntu 22.04 (htc/ubuntu:22.04) Ubuntu 22.04 (Jammy Jellyfish) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__22.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:22.04 Project Website Container Definition Ubuntu 24.04 (htc/ubuntu:24.04) Ubuntu 24.04 (Nobel Numbat) base image OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__ubuntu__24.04.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:24.04 Project Website Container Definition","title":"Base"},{"location":"htc_workloads/containers/available-containers-list/#ai","text":"Tensorflow 2.15 (htc/tensorflow:2.15) Tensorflow image from the Tensorflow project, with OSG additions OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__tensorflow__2.15.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/tensorflow:2.15 Project Website Container Definition scikit-learn:1.3.2 (htc/scikit-learn:1.3) scikit-learn, configured for execution on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__scikit-learn__1.3.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/scikit-learn:1.3 Project Website Container Definition","title":"AI"},{"location":"htc_workloads/containers/available-containers-list/#languages","text":"Julia (opensciencegrid/osgvo-julia) Ubuntu based image with Julia OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.0.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.5.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__1.7.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-julia__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.0.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.5.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:1.7.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:latest Project Website Container Definition Julia (m8zeng/julia-packages) Ubuntu based image with Julia OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/m8zeng__julia-packages__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/m8zeng/julia-packages:latest Project Website Container Definition Matlab Runtime (opensciencegrid/osgvo-matlab-runtime) This is the Matlab runtime component you can use to execute compiled Matlab codes OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2018b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2019a.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2019b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2020a.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2020b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2021b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2022b.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-matlab-runtime__R2023a.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2021b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2022b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2023a Project Website Container Definition Matlab Runtime (htc/matlab-runtime:R2023a) This is the Matlab runtime component you can use to execute compiled Matlab codes OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__matlab-runtime__R2023a.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/matlab-runtime:R2023a Project Website Container Definition R (opensciencegrid/osgvo-r) Example for building R images OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__3.5.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__4.0.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-r__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:4.0.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:latest Project Website Container Definition R (clkwisconsin/spacetimer) Example for building R images OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/clkwisconsin__spacetimer__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/clkwisconsin/spacetimer:latest Project Website Container Definition","title":"Languages"},{"location":"htc_workloads/containers/available-containers-list/#project","text":"XENONnT (opensciencegrid/osgvo-xenon) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.11.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.11.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.12.21.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2020.12.23.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.01.11.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.04.18.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.05.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.06.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.07.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.08.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.08.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.10.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.11.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2021.12.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.01.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.02.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.03.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.04.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.05.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.05.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.06.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.07.27.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.09.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__2022.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__add_latex.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__gpu.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__latex_test3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__py38.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__stable.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__straxen_0-13-1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__straxen_v100.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__switch_deployhq_user.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-xenon__upgrade-boost.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.12.21 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.12.23 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.01.11 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.04.18 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.05.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.06.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.07.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.08.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.08.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.10.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.11.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2021.12.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.01.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.02.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.03.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.04.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.05.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.05.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.5 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.06.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.07.27 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.09.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2022.11.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:add_latex /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:gpu /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:latex_test3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:py38 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:stable /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:straxen_0-13-1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:straxen_v100 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:switch_deployhq_user /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:upgrade-boost Project Website Container Definition XENONnT (xenonnt/base-environment) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.11.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.11.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.21.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.23.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2020.12.24.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.06.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.01.11.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.04.18.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.05.04.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.06.25.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.07.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.08.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.08.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.10.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.11.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2021.12.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.01.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.02.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.03.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.04.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.05.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.05.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.5.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.06.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.07.27.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.09.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__2022.11.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__add_latex.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__gpu.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__latex_test3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__py38.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__stable.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__straxen_v100.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__switch_deployhq_user.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__testing.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__base-environment__upgrade-boost.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.21 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.23 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.12.24 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.04 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.01.11 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.04.18 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.05.04 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.06.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.07.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.08.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.08.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.10.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.11.6 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2021.12.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.01.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.02.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.03.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.04.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.05.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.05.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.2 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.4 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.5 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.06.6 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.07.27 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.09.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2022.11.1 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:add_latex /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:gpu /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:latex_test3 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:py38 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:stable /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:straxen_v100 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:switch_deployhq_user /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:testing /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:upgrade-boost Project Website Container Definition XENONnT (xenonnt/osg_dev) Base software environment for XENONnT, including Python 3.6 and data management tools OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/xenonnt__osg_dev__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/xenonnt/osg_dev:latest Project Website Container Definition","title":"Project"},{"location":"htc_workloads/containers/available-containers-list/#tools","text":"DeepLabCut 3.0.0rc3 (htc/deeplabcut:3.0.0rc4) A software package for animal pose estimation OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__deeplabcut__3.0.0rc4.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/deeplabcut:3.0.0rc4 Project Website Container Definition FreeSurfer (opensciencegrid/osgvo-freesurfer) A software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__6.0.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__6.0.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__7.0.0.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__7.1.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-freesurfer__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.1.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest Project Website Container Definition GROMACS (opensciencegrid/osgvo-gromacs) A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__2018.4.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__2020.2.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:2018.4 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:2020.2 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:latest Project Website Container Definition GROMACS GPU (opensciencegrid/osgvo-gromacs-gpu) A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a GPU enabled version. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-gromacs-gpu__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs-gpu:latest Project Website Container Definition Gromacs 2023.4 (htc/gromacs:2023.4) Gromacs 2023.4 for use on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__gromacs__2023.4.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/gromacs:2023.4 Project Website Container Definition Gromacs 2024.2 (htc/gromacs:2024.2) Gromacs 2024.2 for use on OSG OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__gromacs__2024.2.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/gromacs:2024.2 Project Website Container Definition Minimal (htc/minimal:0) Minimal image - used for testing OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__minimal__0.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/minimal:0 Project Website Container Definition PyTorch 2.3.1 (htc/pytorch:2.3.1-cuda11.8) A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__pytorch__2.3.1-cuda11.8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/htc/pytorch:2.3.1-cuda11.8 Project Website Container Definition Quantum Espresso (opensciencegrid/osgvo-quantum-espresso) A suite for first-principles electronic-structure calculations and materials modeling OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-quantum-espresso__6.6.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-quantum-espresso__6.8.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.8 Project Website Container Definition RASPA2 (opensciencegrid/osgvo-raspa2) General purpose classical simulation package. It can be used for the simulation of molecules in gases, fluids, zeolites, aluminosilicates, metal-organic frameworks, carbon nanotubes and external fields. OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__osgvo-raspa2__2.0.41.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-raspa2:2.0.41 Project Website Container Definition TensorFlow (opensciencegrid/tensorflow) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow__2.3.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest Project Website Container Definition TensorFlow (rynge/tensorflow-cowsay) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/rynge__tensorflow-cowsay__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/rynge/tensorflow-cowsay:latest Project Website Container Definition TensorFlow (jiahe58/tensorflow) TensorFlow image (CPU only) OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/jiahe58__tensorflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/jiahe58/tensorflow:latest Project Website Container Definition TensorFlow GPU (opensciencegrid/tensorflow-gpu) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__2.2-cuda-10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__2.3-cuda-10.1.sif osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/opensciencegrid__tensorflow-gpu__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest Project Website Container Definition TensorFlow GPU (efajardo/astroflow) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/efajardo__astroflow__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/efajardo/astroflow:latest Project Website Container Definition TensorFlow GPU (ssrujanaa/catsanddogs) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/ssrujanaa__catsanddogs__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/ssrujanaa/catsanddogs:latest Project Website Container Definition TensorFlow GPU (weiphy/skopt) TensorFlow image with GPU support OSDF Locations: osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/weiphy__skopt__latest.sif CVMFS Locations: /cvmfs/singularity.opensciencegrid.org/weiphy/skopt:latest Project Website Container Definition","title":"Tools"},{"location":"htc_workloads/containers/containers-docker/","text":"Create/Register a Docker Container Image \u00b6 This guide is meant to accompany the instructions for using containers in the PATh Facility. You can use your own custom container to run jobs in the PATh Facility, and we assume that those containers are built using Docker. This guide describes how to create your own Docker container \"image\" (the blueprint for the container) and how to convert it to a Singularity/Apptainer image. For an overview and how to execute images in PATh, please see Containers - Overview Use an Existing Docker Container \u00b6 If a Docker container image already exists with the software you need, it can be converted to a Singularity/Apptainer image format using this command: apptainer build my-custom-image.sif docker://owner/repo:tag Replace my-custom-image.sif with a name of your choice (keeping the .sif suffix) and the docker://owner/repo:tag can be the identifier for any publicly hosted Docker image, including those on quay.io and the NVIDIA NGC Catalog nvcr.io . Once the .sif file is created, you can copy it to a data directory, test it on the Access Point, and use it in your HTCondor jobs as described in Containers - Overview . Build a Docker Container \u00b6 Install Docker and Get a Docker Hub Account \u00b6 You'll need a Docker Hub account in order to download Docker and share your Docker container images: DockerHub Install Docker Desktop to your computer using the appropriate version for your operating system. Note that PATh does not provide any Docker build hosts. Identify Components \u00b6 What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. Build the Image \u00b6 There are two ways to build a Docker container image: Edit a Dockerfile and use it to produce an image Edit a default image using local Docker We recommend the first option, as it is more reproducible, but the second option can be useful for troubleshooting or especially tricky installs. Option 1: Editing the Dockerfile \u00b6 Create a folder on your computer and inside it, create a blank text file called Dockerfile . The first line of this file should include the keyword FROM and then the name of a Docker image (from Docker Hub) you want to use as your starting point. If using the OSG's Ubuntu Xenial image that would look like this: FROM opensciencegrid/osgvo-ubuntu-xenial Then, for each command you want to run to add libraries or software, use the keyword RUN and then the command. Sometimes it makes sense to string commands together using the && operator and line breaks \\ , like so: RUN apt-get update && \\ apt-get install -yy build-essentials or RUN wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz && \\ tar -xzf R-3.6.0.tar.gz && \\ cd R-3.6.0 && \\ ./configure && \\ make && \\ make install Typically it's good to group together commands installing the same kind of thing (system libraries, or software packages, or an installation process) under one RUN command, and then have multiple RUN commands, one for each of the different type of software or package you're installing. (For all the possible Dockerfile keywords, see the Docker Documentation ) Once your Dockerfile is ready, you can \"build\" the container image by running this command: $ docker build -t namespace/repository_name . Note that the naming convention for Docker images is your Docker Hub username and then a name you choose for that particular container image. So if my Docker Hub username is alice and I created an image with the NCBI blast tool, I might use this name: $ docker build -t alice/NCBI-blast . Option 2: Editing the default image using local Docker \u00b6 You can also build an image interactively, without a Dockerfile. First, get the desired starting image from Docker Hub. Again, we will look at the OSG Ubuntu Xenial image. $ docker pull opensciencegrid/osgvo-ubuntu-xenial We will run the image in a docker interactive session $ docker run -it --name <docker_session_name_here> opensciencegrid/osgvo-ubuntu-xenial /bin/bash Giving the session a name is important because it will make it easier to reattach the session later and commit the changes later on. Now you will be greeted by a new command line prompt that will look something like this [root@740b9db736a1 /]# You can now install the software that you need through the default package manager, in this case apt-get . [root@740b9db736a1 /]# apt-get install build-essentials Once you have installed all the software, you simply exit [root@740b9db736a1 /]# exit Now you can commit the changes to the image and give it a name: docker commit <docker_session_name_here> namespace/repository_name You can also use the session's hash as found in the command prompt ( 740b9db736a1 in the above example) in place of the docker session name. Upload Docker Container to Docker Hub \u00b6 Once your container is complete and tagged, it should appear in the list of local Docker container images, which you can see by running: $ docker images From there, you need to put it in Docker Hub, which can be done via the docker push command: $ docker push namespace/repository_name From here, if you're planning to use this container in OSG, see the first section of this guide . Special Cases \u00b6 Accessing CVMFS \u00b6 If you want your jobs to access CVMFS, make sure that you either: Use one of the base containers provided by the Open Science Pool or Add a /cvmfs folder to your container: If using a Dockerfile, you can do this with the line RUN mkdir /cvmfs If building your container interactively, run $ mkdir -p /cvmfs This will enable the container to access tools and data published on /cvmfs . If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file. ENTRYPOINT and ENV \u00b6 Two options that can be used in the Dockerfile to set the environment or default command are ENTRYPOINT and ENV . Unfortunately, both of these aspects of the Docker container are deleted when it is converted to a Singularity image in the Open Science Pool.","title":"Containers - Docker"},{"location":"htc_workloads/containers/containers-docker/#createregister-a-docker-container-image","text":"This guide is meant to accompany the instructions for using containers in the PATh Facility. You can use your own custom container to run jobs in the PATh Facility, and we assume that those containers are built using Docker. This guide describes how to create your own Docker container \"image\" (the blueprint for the container) and how to convert it to a Singularity/Apptainer image. For an overview and how to execute images in PATh, please see Containers - Overview","title":"Create/Register a Docker Container Image"},{"location":"htc_workloads/containers/containers-docker/#use-an-existing-docker-container","text":"If a Docker container image already exists with the software you need, it can be converted to a Singularity/Apptainer image format using this command: apptainer build my-custom-image.sif docker://owner/repo:tag Replace my-custom-image.sif with a name of your choice (keeping the .sif suffix) and the docker://owner/repo:tag can be the identifier for any publicly hosted Docker image, including those on quay.io and the NVIDIA NGC Catalog nvcr.io . Once the .sif file is created, you can copy it to a data directory, test it on the Access Point, and use it in your HTCondor jobs as described in Containers - Overview .","title":"Use an Existing Docker Container"},{"location":"htc_workloads/containers/containers-docker/#build-a-docker-container","text":"","title":"Build a Docker Container"},{"location":"htc_workloads/containers/containers-docker/#install-docker-and-get-a-docker-hub-account","text":"You'll need a Docker Hub account in order to download Docker and share your Docker container images: DockerHub Install Docker Desktop to your computer using the appropriate version for your operating system. Note that PATh does not provide any Docker build hosts.","title":"Install Docker and Get a Docker Hub Account"},{"location":"htc_workloads/containers/containers-docker/#identify-components","text":"What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name.","title":"Identify Components"},{"location":"htc_workloads/containers/containers-docker/#build-the-image","text":"There are two ways to build a Docker container image: Edit a Dockerfile and use it to produce an image Edit a default image using local Docker We recommend the first option, as it is more reproducible, but the second option can be useful for troubleshooting or especially tricky installs.","title":"Build the Image"},{"location":"htc_workloads/containers/containers-docker/#option-1-editing-the-dockerfile","text":"Create a folder on your computer and inside it, create a blank text file called Dockerfile . The first line of this file should include the keyword FROM and then the name of a Docker image (from Docker Hub) you want to use as your starting point. If using the OSG's Ubuntu Xenial image that would look like this: FROM opensciencegrid/osgvo-ubuntu-xenial Then, for each command you want to run to add libraries or software, use the keyword RUN and then the command. Sometimes it makes sense to string commands together using the && operator and line breaks \\ , like so: RUN apt-get update && \\ apt-get install -yy build-essentials or RUN wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz && \\ tar -xzf R-3.6.0.tar.gz && \\ cd R-3.6.0 && \\ ./configure && \\ make && \\ make install Typically it's good to group together commands installing the same kind of thing (system libraries, or software packages, or an installation process) under one RUN command, and then have multiple RUN commands, one for each of the different type of software or package you're installing. (For all the possible Dockerfile keywords, see the Docker Documentation ) Once your Dockerfile is ready, you can \"build\" the container image by running this command: $ docker build -t namespace/repository_name . Note that the naming convention for Docker images is your Docker Hub username and then a name you choose for that particular container image. So if my Docker Hub username is alice and I created an image with the NCBI blast tool, I might use this name: $ docker build -t alice/NCBI-blast .","title":"Option 1: Editing the Dockerfile"},{"location":"htc_workloads/containers/containers-docker/#option-2-editing-the-default-image-using-local-docker","text":"You can also build an image interactively, without a Dockerfile. First, get the desired starting image from Docker Hub. Again, we will look at the OSG Ubuntu Xenial image. $ docker pull opensciencegrid/osgvo-ubuntu-xenial We will run the image in a docker interactive session $ docker run -it --name <docker_session_name_here> opensciencegrid/osgvo-ubuntu-xenial /bin/bash Giving the session a name is important because it will make it easier to reattach the session later and commit the changes later on. Now you will be greeted by a new command line prompt that will look something like this [root@740b9db736a1 /]# You can now install the software that you need through the default package manager, in this case apt-get . [root@740b9db736a1 /]# apt-get install build-essentials Once you have installed all the software, you simply exit [root@740b9db736a1 /]# exit Now you can commit the changes to the image and give it a name: docker commit <docker_session_name_here> namespace/repository_name You can also use the session's hash as found in the command prompt ( 740b9db736a1 in the above example) in place of the docker session name.","title":"Option 2: Editing the default image using local Docker"},{"location":"htc_workloads/containers/containers-docker/#upload-docker-container-to-docker-hub","text":"Once your container is complete and tagged, it should appear in the list of local Docker container images, which you can see by running: $ docker images From there, you need to put it in Docker Hub, which can be done via the docker push command: $ docker push namespace/repository_name From here, if you're planning to use this container in OSG, see the first section of this guide .","title":"Upload Docker Container to Docker Hub"},{"location":"htc_workloads/containers/containers-docker/#special-cases","text":"","title":"Special Cases"},{"location":"htc_workloads/containers/containers-docker/#accessing-cvmfs","text":"If you want your jobs to access CVMFS, make sure that you either: Use one of the base containers provided by the Open Science Pool or Add a /cvmfs folder to your container: If using a Dockerfile, you can do this with the line RUN mkdir /cvmfs If building your container interactively, run $ mkdir -p /cvmfs This will enable the container to access tools and data published on /cvmfs . If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file.","title":"Accessing CVMFS"},{"location":"htc_workloads/containers/containers-docker/#entrypoint-and-env","text":"Two options that can be used in the Dockerfile to set the environment or default command are ENTRYPOINT and ENV . Unfortunately, both of these aspects of the Docker container are deleted when it is converted to a Singularity image in the Open Science Pool.","title":"ENTRYPOINT and ENV"},{"location":"htc_workloads/containers/containers-singularity/","text":"Apptainer/Singularity Container Images \u00b6 This guide describes how to create your own Apptainer/Singularity container \"image\" (the blueprint for the container). For an overview and how to execute images on the PATh Facility, please see Containers - Overview Identify Components \u00b6 What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. Editing the Build Spec \u00b6 Create a folder on your computer and inside it, create a blank text file called image.def . The first lines of this file should include where to get the base image from. If using the OSG's Ubuntu 20.04 image that would look like this: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest Then there is a section called %post where you put the additional commands to make the image just like you need it. For example: %post apt-get update -y apt-get install -y \\ build-essential \\ cmake \\ g++ \\ r-base-dev R -e \"install.packages('cowsay', dependencies=TRUE, repos='http://cran.rstudio.com/')\" See the Apptainer documentation for a full reference on how to specify build specs. Note that the %runscript section is ignored when the container is executed under the PATh Facility. The final image.def looks like: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest %post apt-get update -y apt-get install -y \\ build-essential \\ cmake \\ g++ \\ r-base-dev R -e \"install.packages('cowsay', dependencies=TRUE, repos='http://cran.rstudio.com/')\" Once your build spec is ready, you can \"build\" the container image by running this command: $ apptainer build my-container.sif image.def Once the image is built, you can copy it to a data directory, test it on the Access Point, and use it in your HTCondor jobs. This is all described in Containers - Overview .","title":"Containers - Apptainer/Singularity"},{"location":"htc_workloads/containers/containers-singularity/#apptainersingularity-container-images","text":"This guide describes how to create your own Apptainer/Singularity container \"image\" (the blueprint for the container). For an overview and how to execute images on the PATh Facility, please see Containers - Overview","title":"Apptainer/Singularity Container Images"},{"location":"htc_workloads/containers/containers-singularity/#identify-components","text":"What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name.","title":"Identify Components"},{"location":"htc_workloads/containers/containers-singularity/#editing-the-build-spec","text":"Create a folder on your computer and inside it, create a blank text file called image.def . The first lines of this file should include where to get the base image from. If using the OSG's Ubuntu 20.04 image that would look like this: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest Then there is a section called %post where you put the additional commands to make the image just like you need it. For example: %post apt-get update -y apt-get install -y \\ build-essential \\ cmake \\ g++ \\ r-base-dev R -e \"install.packages('cowsay', dependencies=TRUE, repos='http://cran.rstudio.com/')\" See the Apptainer documentation for a full reference on how to specify build specs. Note that the %runscript section is ignored when the container is executed under the PATh Facility. The final image.def looks like: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest %post apt-get update -y apt-get install -y \\ build-essential \\ cmake \\ g++ \\ r-base-dev R -e \"install.packages('cowsay', dependencies=TRUE, repos='http://cran.rstudio.com/')\" Once your build spec is ready, you can \"build\" the container image by running this command: $ apptainer build my-container.sif image.def Once the image is built, you can copy it to a data directory, test it on the Access Point, and use it in your HTCondor jobs. This is all described in Containers - Overview .","title":"Editing the Build Spec"},{"location":"htc_workloads/containers/containers/","text":"Software Containers \u00b6 Docker and Apptainer/Singularity are container systems that allow users full control over their software environment. You can create your own container image or choose from a set of pre-defined images, and specify that your submitted jobs run within one of these. For jobs on the PATh Facility, it does not matter whether you provide a Docker or Apptainer/Singularity image. Either is compatible with our system and can be used with minimal to no modification. Determining factors on when to use Apptainer/Singularity images over Docker images include if an image already exists, external to PATh distribution preferences, and if you have experience building images in one for format and not the other. Because the PATh Facility is a distributed infrastructure and workloads consists of a large number jobs (and there container executions), it is important to consider how the container image is transferred to the execution nodes. The instructions below contain best practices when it comes to access both Apptainer/Singularity and Docker images. When using a container for your jobs, the container image is automatically started up when HTCondor matches your job to a slot. The executable provided in the submit script will be run within the context of the container image, having access to software and libraries that were installed to the image, as if they were already on the server where the job is running. Job executables need not (and should not) run any commands to start the container. Nor should the container image contain any entrypoint/cmd - the job is the command to be run in the container. PATh-Provided Images \u00b6 The Facilition Team maintains a set of images that are already in a shared Apptainer/Singularity repository. A list of available containers can be found on this page . If the software you need isn't already supported in a listed container, you can use your own container or any container image in Docker Hub (see sections further below). Once the container you need is in the Apptainer/Singularity repository, your can submit jobs that run within a particular container by listing the container image in the submit file. container_image = osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9.sif <other usual submit file lines> queue Custom Apptainer Images \u00b6 If you already have software in the form of a .sif Apptainer/Singuilarity file, you can stage the .sif file with your job. For small workloads, sending the file with the job is ok: container_image = my-custom-image-v1.sif There is no need to list the image under transfer_input_files - the submit process will do this, just like it does for your executable. For larger workloads, the image will be resused for each job, and thus the preferred transfer method is an OSDF tool. Store the .sif file under /path-facility/data/$USER/ , and then use the OSDF url directly in the container_image attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual PATh Facility username. Example: container_image = osdf:///path-facility/data/USERNAME/my-custom-image-v1.sif <other usual submit file lines> queue Be aware that the OSDF aggressively caches the image based on file naming. If you need to do quick changes, please use versioning of the .sif file so that the caches see a \"new\" name. In this example, replacing my-custom-image-v1.sif with new content will probably mean that some nodes get the old version and some nodes the new version. Prevent this by creating a new file named with v2. More information on how to create Apptainer/Singularity images can be found in the Apptainer Images Guide . If you already have a Docker image, or want to build one, see our Docker Images Guide . Migrating from +SingularityImage \u00b6 Historically, the PATh Facility used the custom attribute +SingularityImage to specify container images. This has now been replaced by the standard container_image attribute. When migrating, note a key difference in syntax: +SingularityImage requires the image path to be enclosed in double quotes. container_image must be specified without quotes.","title":"Containers - Overview"},{"location":"htc_workloads/containers/containers/#software-containers","text":"Docker and Apptainer/Singularity are container systems that allow users full control over their software environment. You can create your own container image or choose from a set of pre-defined images, and specify that your submitted jobs run within one of these. For jobs on the PATh Facility, it does not matter whether you provide a Docker or Apptainer/Singularity image. Either is compatible with our system and can be used with minimal to no modification. Determining factors on when to use Apptainer/Singularity images over Docker images include if an image already exists, external to PATh distribution preferences, and if you have experience building images in one for format and not the other. Because the PATh Facility is a distributed infrastructure and workloads consists of a large number jobs (and there container executions), it is important to consider how the container image is transferred to the execution nodes. The instructions below contain best practices when it comes to access both Apptainer/Singularity and Docker images. When using a container for your jobs, the container image is automatically started up when HTCondor matches your job to a slot. The executable provided in the submit script will be run within the context of the container image, having access to software and libraries that were installed to the image, as if they were already on the server where the job is running. Job executables need not (and should not) run any commands to start the container. Nor should the container image contain any entrypoint/cmd - the job is the command to be run in the container.","title":"Software Containers"},{"location":"htc_workloads/containers/containers/#path-provided-images","text":"The Facilition Team maintains a set of images that are already in a shared Apptainer/Singularity repository. A list of available containers can be found on this page . If the software you need isn't already supported in a listed container, you can use your own container or any container image in Docker Hub (see sections further below). Once the container you need is in the Apptainer/Singularity repository, your can submit jobs that run within a particular container by listing the container image in the submit file. container_image = osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64/htc__rocky__9.sif <other usual submit file lines> queue","title":"PATh-Provided Images"},{"location":"htc_workloads/containers/containers/#custom-apptainer-images","text":"If you already have software in the form of a .sif Apptainer/Singuilarity file, you can stage the .sif file with your job. For small workloads, sending the file with the job is ok: container_image = my-custom-image-v1.sif There is no need to list the image under transfer_input_files - the submit process will do this, just like it does for your executable. For larger workloads, the image will be resused for each job, and thus the preferred transfer method is an OSDF tool. Store the .sif file under /path-facility/data/$USER/ , and then use the OSDF url directly in the container_image attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual PATh Facility username. Example: container_image = osdf:///path-facility/data/USERNAME/my-custom-image-v1.sif <other usual submit file lines> queue Be aware that the OSDF aggressively caches the image based on file naming. If you need to do quick changes, please use versioning of the .sif file so that the caches see a \"new\" name. In this example, replacing my-custom-image-v1.sif with new content will probably mean that some nodes get the old version and some nodes the new version. Prevent this by creating a new file named with v2. More information on how to create Apptainer/Singularity images can be found in the Apptainer Images Guide . If you already have a Docker image, or want to build one, see our Docker Images Guide .","title":"Custom Apptainer Images"},{"location":"htc_workloads/containers/containers/#migrating-from-singularityimage","text":"Historically, the PATh Facility used the custom attribute +SingularityImage to specify container images. This has now been replaced by the standard container_image attribute. When migrating, note a key difference in syntax: +SingularityImage requires the image path to be enclosed in double quotes. container_image must be specified without quotes.","title":"Migrating from +SingularityImage"},{"location":"htc_workloads/specific_resources/gpus/","text":"GPUs \u00b6 The PATh Facility has an increasing number of GPUs available to run jobs. Requesting GPUs \u00b6 To request a GPU for your HTCondor job, you can use the HTCondor request_gpus attribute in your submit file (along with the usual request_cpus , request_memory , and request_disk attributes). For example: request_gpus = 1 request_cpus = 1 request_memory = 4 GB request_disk = 2 GB You should only request a GPU if your software has been written to use a GPU. Specific GPU Requests \u00b6 If your software or code requires a certain type of GPU, or has some other special requirement, there are options to specify this. gpus_minimum_capability = <version> gpus_maximum_capability = <version> gpus_minimum_memory = <quantity in MB> The first two options relate to GPU capability; the last to GPU memory 1) Capability If you want a certain type or family of GPUs, we usually recommend using the GPU's \"Capability\". This value is NOT the CUDA library, but rather a measure of the GPU's \"Compute Capability,\" which is related to hardware generation. See the table below for examples of GPU capability values. For example, an NVIDIA A100 GPU has a Compute Capability of 8.0, so if you wanted to run on an A100 GPU specifically, the submit file requirement would be: gpus_minimum_capability = 8.0 If you wanted only to use that type of GPU (not anything newer), you could also set the maximum capability: gpus_maximum_capability = 8.0 2) GPU Memory GPU memory (also sometimes called \"vram\") is the amount of memory available on the GPU device. In HTCondor, GPU Memory is in units of megabytes. If you didn't care about the age of the GPU, but just the amount of available memory (at least, say 20GB or 20,000MB), you would use something like: gpus_minimum_memory = 20000 There are additional attributes that can be used to select GPU types; email the facilitation team if the above options do not satisfy your use case. Note that the more requirements you include, the fewer resources will be available to you! It's always better to set the minimal possible requirements (ideally, none!) in order to access the greatest amount of computing capacity. Available GPUs \u00b6 Description of the available CPUs can be found under the Facility Description . Currently, A100 is the main GPU available. All of the A100s in the Facility have 40GB of GPU memory and are Capability 8.0. Software and Data Considerations \u00b6 Software for GPUs \u00b6 For GPU-enabled machine learning libraries, we recommend using containers to set up your software for jobs: Software Containers","title":"GPUs"},{"location":"htc_workloads/specific_resources/gpus/#gpus","text":"The PATh Facility has an increasing number of GPUs available to run jobs.","title":"GPUs"},{"location":"htc_workloads/specific_resources/gpus/#requesting-gpus","text":"To request a GPU for your HTCondor job, you can use the HTCondor request_gpus attribute in your submit file (along with the usual request_cpus , request_memory , and request_disk attributes). For example: request_gpus = 1 request_cpus = 1 request_memory = 4 GB request_disk = 2 GB You should only request a GPU if your software has been written to use a GPU.","title":"Requesting GPUs"},{"location":"htc_workloads/specific_resources/gpus/#specific-gpu-requests","text":"If your software or code requires a certain type of GPU, or has some other special requirement, there are options to specify this. gpus_minimum_capability = <version> gpus_maximum_capability = <version> gpus_minimum_memory = <quantity in MB> The first two options relate to GPU capability; the last to GPU memory 1) Capability If you want a certain type or family of GPUs, we usually recommend using the GPU's \"Capability\". This value is NOT the CUDA library, but rather a measure of the GPU's \"Compute Capability,\" which is related to hardware generation. See the table below for examples of GPU capability values. For example, an NVIDIA A100 GPU has a Compute Capability of 8.0, so if you wanted to run on an A100 GPU specifically, the submit file requirement would be: gpus_minimum_capability = 8.0 If you wanted only to use that type of GPU (not anything newer), you could also set the maximum capability: gpus_maximum_capability = 8.0 2) GPU Memory GPU memory (also sometimes called \"vram\") is the amount of memory available on the GPU device. In HTCondor, GPU Memory is in units of megabytes. If you didn't care about the age of the GPU, but just the amount of available memory (at least, say 20GB or 20,000MB), you would use something like: gpus_minimum_memory = 20000 There are additional attributes that can be used to select GPU types; email the facilitation team if the above options do not satisfy your use case. Note that the more requirements you include, the fewer resources will be available to you! It's always better to set the minimal possible requirements (ideally, none!) in order to access the greatest amount of computing capacity.","title":"Specific GPU Requests"},{"location":"htc_workloads/specific_resources/gpus/#available-gpus","text":"Description of the available CPUs can be found under the Facility Description . Currently, A100 is the main GPU available. All of the A100s in the Facility have 40GB of GPU memory and are Capability 8.0.","title":"Available GPUs"},{"location":"htc_workloads/specific_resources/gpus/#software-and-data-considerations","text":"","title":"Software and Data Considerations"},{"location":"htc_workloads/specific_resources/gpus/#software-for-gpus","text":"For GPU-enabled machine learning libraries, we recommend using containers to set up your software for jobs: Software Containers","title":"Software for GPUs"},{"location":"htc_workloads/specific_resources/large-memory/","text":"Large Memory Jobs \u00b6 By default, 2 GB of RAM (aka memory) will be assigned to your jobs. However, some jobs will require additional memory to complete successfully. To request more memory, use the HTCondor request_memory attribute in your submit file. The default unit is MB. For example, the following will request 12 GB: request_memory = 12228 You might be wondering why the above is requesting 12228 MB for 12 GB. That's because byte units don't actually scale by 1000 (10^10) like the metric system, but instead scale by 1024 (2^10) due to the binary nature of bytes. Alternatively, you can define a memory request using standard units request_memory = 12GB We recommend always explictly defining the byte units in your request_memory statement.","title":"Large Memory"},{"location":"htc_workloads/specific_resources/large-memory/#large-memory-jobs","text":"By default, 2 GB of RAM (aka memory) will be assigned to your jobs. However, some jobs will require additional memory to complete successfully. To request more memory, use the HTCondor request_memory attribute in your submit file. The default unit is MB. For example, the following will request 12 GB: request_memory = 12228 You might be wondering why the above is requesting 12228 MB for 12 GB. That's because byte units don't actually scale by 1000 (10^10) like the metric system, but instead scale by 1024 (2^10) due to the binary nature of bytes. Alternatively, you can define a memory request using standard units request_memory = 12GB We recommend always explictly defining the byte units in your request_memory statement.","title":"Large Memory Jobs"},{"location":"htc_workloads/specific_resources/multicore/","text":"Multicore Jobs \u00b6 Multicore jobs can be submitted for threaded or OpenMP applications. To request multiple cores (aka cpus) use the HTCondor request_cpus attribute in your submit file. Example: request_cpus = 8 We recommend requesting a maximum of 8 cpus. Important considerations When submitting multicore jobs please note that you will also have to tell your code or application to use the number of cpus requested in your submit file. Do not use core auto-detection as it might detect more cores than what were actually assigned to your job.","title":"Multicore"},{"location":"htc_workloads/specific_resources/multicore/#multicore-jobs","text":"Multicore jobs can be submitted for threaded or OpenMP applications. To request multiple cores (aka cpus) use the HTCondor request_cpus attribute in your submit file. Example: request_cpus = 8 We recommend requesting a maximum of 8 cpus. Important considerations When submitting multicore jobs please note that you will also have to tell your code or application to use the number of cpus requested in your submit file. Do not use core auto-detection as it might detect more cores than what were actually assigned to your job.","title":"Multicore Jobs"},{"location":"htc_workloads/submitting_workloads/checkpointing/","text":"Checkpointing Jobs \u00b6 What is Checkpointing? \u00b6 Checkpointing is a technique that provides fault tolerance for a user's analysis. It consists of saving snapshots of a job's progress so the job can be restarted without losing its progress and having to restart from the beginning. We highly encourage checkpointing as a solution for long jobs. This section is about jobs capable of periodically saving checkpoint information, and how to make HTCondor store that information safely, in case it's needed to continue the job on another machine or at a later time. There are two types of checkpointing: exit driven and eviction driven. In a vast majority of cases, exit driven checkpointing is preferred over eviction driven checkpointing. Therefore, this guide will focus on how to utilize exit driven checkpointing for your analysis. Note that not all software, programs, or code are capable of creating checkpoint files and knowing how to resume from them. Consult the manual for your software or program to determine if it supports checkpointing features. Some manuals will refer this ability as \"checkpoint\" features, as the ability to \"resume\" mid-analysis if a job is interrupted, or as \"checkpoint/restart\" capabilities. Contact a Research Computing Facilitator if you would like help determining if your software, program, or code is able to checkpoint. Why Checkpoint? \u00b6 Checkpointing allows a job to automatically resume from approximately where it left off instead of having to start over if interrupted. This behavior is advantageous for jobs limited by a maximum runtime policy. It is also advantageous for jobs submitted to backfill resources with no runtime guarantee (i.e. jobs on the OSPool) where the compute resources may also be more prone to hardware or networking failures. For example, checkpointing jobs that are limited by a runtime policy can enable HTCondor to exit a job and automatically requeue it to avoid hitting the maximum runtime limit. By using checkpointing, jobs circumvent hitting the maximum runtime limit and can run for extended periods of time until the completion of the analysis. This behavior avoids costly setbacks that may be caused by loosing results mid-way through an analysis due to hitting a runtime limit. Process of Exit Driven Checkpointing \u00b6 Using exit driven checkpointing, a job is specified to time out after a user-specified amount of time with an exit code value of 85 (more on this below). Upon hitting this time limit, HTCondor transfers any checkpoint files listed in the submit file attribute transfer_checkpoint_files to a directory called /spool . This directory acts as a storage location for these files in case the job is interrupted. HTCondor then knows that jobs with exit code 85 should be automatically requeued, and will transfer the checkpoint files in /spool to your job's working directory prior to restarting your executable. The process of exit driven checkpointing relies heavily on the use of exit codes to determine the next appropriate steps for HTCondor to take with a job. In general, exit codes are used to report system responses, such as when an analysis is running, encountered an error, or successfully completes. HTCondor recognizes exit code 85 as checkpointing jobs and therefore will know to handle these jobs differently than non-checkpoiting jobs. Requirements for Exit Driven Checkpointing \u00b6 Requirements for your code or software: Checkpoint : The software, program, or code you are using must be able to capture checkpoint files (i.e. snapshots of the progress made thus far) and know how to resume from them. Resume : This means your code must be able to recognize checkpoint files and know to resume from them instead of the original input data when the code is restarted. Exit : Jobs should exit with an exit code value of 85 after successfully creating checkpoint files. Additionally, jobs need to be able to exit with a non- 85 value if they encounter an error or write the writing the final outputs. In some cases, these requirements can be achieved by using a wrapper script. This means that your executable may be a script, rather than the code that is writing the checkpoint. An example wrapper script that enables some of these behaviors is below. Contact a Research Computing Facilitator for help determining if your job is capable of using checkpointing. Changes to the Submit File \u00b6 Several modifications to the submit file are needed to enable HTCondor's checkpointing feature. The line checkpoint_exit_code = 85 must be added. HTCondor recognizes code 85 as a checkpoint job. This means HTCondor knows to end a job with this code but to then to requeue it repeatedly until the analysis completes. The value of when_to_transfer_output should be set to ON_EXIT . The name of the checkpoint files or directories to be transferred to /spool should be specified using transfer_checkpoint_files . Optional In some cases, it is necessary to write a wrapper script to tell a job when to timeout and exit. In cases such as this, the executable will need to be changed to the name of that wrapper script. An example of a wrapper script that enables a job to checkout and exit with the proper exit codes can be found below. An example submit file for an exit driven checkpointing job looks like: # exit-driven-example.submit executable = exit-driven.sh arguments = argument1 argument2 checkpoint_exit_code = 85 transfer_checkpoint_files = my_output.txt, temp_dir, temp_file.txt should_transfer_files = yes when_to_transfer_output = ON_EXIT output = example.out error = example.err log = example.log cpu = 1 request_disk = 2 GB request_memory = 2 GB queue 1 Example Wrapper Script for Checkpointing Job \u00b6 As previously described, it may be necessary to use a wrapper script to tell your job when and how to exit as it checkpoints. An example of a wrapper script that tells a job to exit every 4 hours looks like: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash timeout 4h do_science arg1 arg2 timeout_exit_status = $? if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status Let's take a moment to understand what each section of this wrapper script is doing: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash timeout 4h do_science argument1 argument2 # The `timeout` command will stop the job after 4 hours (4h). # This number can be increased or decreased depending on how frequent your code/software/program # is creating checkpoint files and how long it takes to create/resume from these files. # Replace `do_science argument1 argument2` with the execution command and arguments for your job. timeout_exit_status = $? # Uses the bash notation of `$?` to call the exit value of the last executed command # and to save it in a variable called `timeout_exit_status`. if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status # Programs typically have an exit code of `124` while they are actively running. # The portion above replaces exit code `124` with code `85`. HTCondor recognizes # code `85` and knows to end a job with this code once the time specified by `timeout` # has been reached. Upon exiting, HTCondor saves the files from jobs with exit code `85` # in the temporary directory within `/spool`. Once the files have been transferred, # HTCondor automatically requeues that job and fetches the files found in `/spool`. # If an exit code of `124` is not observed (for example if the program is done running # or has encountered an error), HTCondor will end the job and will not automaticlally requeue it. The ideal timeout frequency for a job is every 1-5 hours with a maximum of 10 hours. For jobs that checkpoint and timeout in under an hour, it is possible that a job may spend more time with checkpointing procedures than moving forward with the analysis. After 10 hours, the likelihood of a job being inturrupted on the OSPool is higher. Checking the Progress of Checkpointing Jobs \u00b6 It is possible to investigate checkpoint files once they have been transferred to /spool . You can explore the checkpointed files in /spool by navigating to /home/condor/spool on an OSPool Access Point. The directories in this folder are the last four digits of a job's cluster ID with leading zeros removed. Sub folders are labeled with the process ID for each job. For example, to investigate the checkpoint files for 17870068.220 , the files in /spool would be found in folder 68 in a subdirectory called 220 . More Information \u00b6 More information on checkpointing HTCondor jobs can be found in HTCondor's manual: https://htcondor.readthedocs.io/en/latest/users-manual/self-checkpointing-applications.html This documentation contains additional features available to checkpointing jobs, as well as additional examples such as a python checkpointing job.","title":"Checkpointing Jobs"},{"location":"htc_workloads/submitting_workloads/checkpointing/#checkpointing-jobs","text":"","title":"Checkpointing Jobs"},{"location":"htc_workloads/submitting_workloads/checkpointing/#what-is-checkpointing","text":"Checkpointing is a technique that provides fault tolerance for a user's analysis. It consists of saving snapshots of a job's progress so the job can be restarted without losing its progress and having to restart from the beginning. We highly encourage checkpointing as a solution for long jobs. This section is about jobs capable of periodically saving checkpoint information, and how to make HTCondor store that information safely, in case it's needed to continue the job on another machine or at a later time. There are two types of checkpointing: exit driven and eviction driven. In a vast majority of cases, exit driven checkpointing is preferred over eviction driven checkpointing. Therefore, this guide will focus on how to utilize exit driven checkpointing for your analysis. Note that not all software, programs, or code are capable of creating checkpoint files and knowing how to resume from them. Consult the manual for your software or program to determine if it supports checkpointing features. Some manuals will refer this ability as \"checkpoint\" features, as the ability to \"resume\" mid-analysis if a job is interrupted, or as \"checkpoint/restart\" capabilities. Contact a Research Computing Facilitator if you would like help determining if your software, program, or code is able to checkpoint.","title":"What is Checkpointing?"},{"location":"htc_workloads/submitting_workloads/checkpointing/#why-checkpoint","text":"Checkpointing allows a job to automatically resume from approximately where it left off instead of having to start over if interrupted. This behavior is advantageous for jobs limited by a maximum runtime policy. It is also advantageous for jobs submitted to backfill resources with no runtime guarantee (i.e. jobs on the OSPool) where the compute resources may also be more prone to hardware or networking failures. For example, checkpointing jobs that are limited by a runtime policy can enable HTCondor to exit a job and automatically requeue it to avoid hitting the maximum runtime limit. By using checkpointing, jobs circumvent hitting the maximum runtime limit and can run for extended periods of time until the completion of the analysis. This behavior avoids costly setbacks that may be caused by loosing results mid-way through an analysis due to hitting a runtime limit.","title":"Why Checkpoint?"},{"location":"htc_workloads/submitting_workloads/checkpointing/#process-of-exit-driven-checkpointing","text":"Using exit driven checkpointing, a job is specified to time out after a user-specified amount of time with an exit code value of 85 (more on this below). Upon hitting this time limit, HTCondor transfers any checkpoint files listed in the submit file attribute transfer_checkpoint_files to a directory called /spool . This directory acts as a storage location for these files in case the job is interrupted. HTCondor then knows that jobs with exit code 85 should be automatically requeued, and will transfer the checkpoint files in /spool to your job's working directory prior to restarting your executable. The process of exit driven checkpointing relies heavily on the use of exit codes to determine the next appropriate steps for HTCondor to take with a job. In general, exit codes are used to report system responses, such as when an analysis is running, encountered an error, or successfully completes. HTCondor recognizes exit code 85 as checkpointing jobs and therefore will know to handle these jobs differently than non-checkpoiting jobs.","title":"Process of Exit Driven Checkpointing"},{"location":"htc_workloads/submitting_workloads/checkpointing/#requirements-for-exit-driven-checkpointing","text":"Requirements for your code or software: Checkpoint : The software, program, or code you are using must be able to capture checkpoint files (i.e. snapshots of the progress made thus far) and know how to resume from them. Resume : This means your code must be able to recognize checkpoint files and know to resume from them instead of the original input data when the code is restarted. Exit : Jobs should exit with an exit code value of 85 after successfully creating checkpoint files. Additionally, jobs need to be able to exit with a non- 85 value if they encounter an error or write the writing the final outputs. In some cases, these requirements can be achieved by using a wrapper script. This means that your executable may be a script, rather than the code that is writing the checkpoint. An example wrapper script that enables some of these behaviors is below. Contact a Research Computing Facilitator for help determining if your job is capable of using checkpointing.","title":"Requirements for Exit Driven Checkpointing"},{"location":"htc_workloads/submitting_workloads/checkpointing/#changes-to-the-submit-file","text":"Several modifications to the submit file are needed to enable HTCondor's checkpointing feature. The line checkpoint_exit_code = 85 must be added. HTCondor recognizes code 85 as a checkpoint job. This means HTCondor knows to end a job with this code but to then to requeue it repeatedly until the analysis completes. The value of when_to_transfer_output should be set to ON_EXIT . The name of the checkpoint files or directories to be transferred to /spool should be specified using transfer_checkpoint_files . Optional In some cases, it is necessary to write a wrapper script to tell a job when to timeout and exit. In cases such as this, the executable will need to be changed to the name of that wrapper script. An example of a wrapper script that enables a job to checkout and exit with the proper exit codes can be found below. An example submit file for an exit driven checkpointing job looks like: # exit-driven-example.submit executable = exit-driven.sh arguments = argument1 argument2 checkpoint_exit_code = 85 transfer_checkpoint_files = my_output.txt, temp_dir, temp_file.txt should_transfer_files = yes when_to_transfer_output = ON_EXIT output = example.out error = example.err log = example.log cpu = 1 request_disk = 2 GB request_memory = 2 GB queue 1","title":"Changes to the Submit File"},{"location":"htc_workloads/submitting_workloads/checkpointing/#example-wrapper-script-for-checkpointing-job","text":"As previously described, it may be necessary to use a wrapper script to tell your job when and how to exit as it checkpoints. An example of a wrapper script that tells a job to exit every 4 hours looks like: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash timeout 4h do_science arg1 arg2 timeout_exit_status = $? if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status Let's take a moment to understand what each section of this wrapper script is doing: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash timeout 4h do_science argument1 argument2 # The `timeout` command will stop the job after 4 hours (4h). # This number can be increased or decreased depending on how frequent your code/software/program # is creating checkpoint files and how long it takes to create/resume from these files. # Replace `do_science argument1 argument2` with the execution command and arguments for your job. timeout_exit_status = $? # Uses the bash notation of `$?` to call the exit value of the last executed command # and to save it in a variable called `timeout_exit_status`. if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status # Programs typically have an exit code of `124` while they are actively running. # The portion above replaces exit code `124` with code `85`. HTCondor recognizes # code `85` and knows to end a job with this code once the time specified by `timeout` # has been reached. Upon exiting, HTCondor saves the files from jobs with exit code `85` # in the temporary directory within `/spool`. Once the files have been transferred, # HTCondor automatically requeues that job and fetches the files found in `/spool`. # If an exit code of `124` is not observed (for example if the program is done running # or has encountered an error), HTCondor will end the job and will not automaticlally requeue it. The ideal timeout frequency for a job is every 1-5 hours with a maximum of 10 hours. For jobs that checkpoint and timeout in under an hour, it is possible that a job may spend more time with checkpointing procedures than moving forward with the analysis. After 10 hours, the likelihood of a job being inturrupted on the OSPool is higher.","title":"Example Wrapper Script for Checkpointing Job"},{"location":"htc_workloads/submitting_workloads/checkpointing/#checking-the-progress-of-checkpointing-jobs","text":"It is possible to investigate checkpoint files once they have been transferred to /spool . You can explore the checkpointed files in /spool by navigating to /home/condor/spool on an OSPool Access Point. The directories in this folder are the last four digits of a job's cluster ID with leading zeros removed. Sub folders are labeled with the process ID for each job. For example, to investigate the checkpoint files for 17870068.220 , the files in /spool would be found in folder 68 in a subdirectory called 220 .","title":"Checking the Progress of Checkpointing Jobs"},{"location":"htc_workloads/submitting_workloads/checkpointing/#more-information","text":"More information on checkpointing HTCondor jobs can be found in HTCondor's manual: https://htcondor.readthedocs.io/en/latest/users-manual/self-checkpointing-applications.html This documentation contains additional features available to checkpointing jobs, as well as additional examples such as a python checkpointing job.","title":"More Information"},{"location":"htc_workloads/submitting_workloads/quickstart/","text":"Quickstart - Submit Example HTCondor Jobs \u00b6 Job 1: A simple, nonparallel job \u00b6 First, using a text editing program (e.g. Nano, Vim), create a file called short.sh to use as an executable for our sample job: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # short.sh: a short discovery job set -e printf \"Start time: \" ; /bin/date printf \"Job is running on node: \" ; /bin/hostname printf \"Job running as user: \" ; /usr/bin/id printf \"Job is running in directory: \" ; /bin/pwd echo echo \"Working hard...\" sleep 20 echo \"Science complete!\" Now, make the script executable. $ chmod +x short.sh Run the job locally \u00b6 When possible, it is important to first test your job on your local resorces prior to submitting the job to HTCondor to run on the PATh Facility execution points. This will help identify potential errors within the code prior to queuing many HTCondor jobs. For example, we can run this executable on our local terminal by typing: $ ./short.sh Start time: Wed Aug 21 09:21:35 CDT 2013 Job is running on node: ap1.path-facility Job running as user: uid=54161(username) gid=1000(users) Job is running in directory: /home/username/quickstart Working hard... Science complete! Create an HTCondor submit file \u00b6 So far, so good! Let's create a simple (if verbose) HTCondor submit file by using a text editing program. Name it tutorial01.submit . # Our executable is the main program or script that we've created # to do the 'work' of a single job. executable = short.sh # We need to name the files that HTCondor should create to save the # terminal output (stdout) and error (stderr) created by our job. # Similarly, we need to name the log file where HTCondor will save # information about job execution steps. error = short.error output = short.output log = short.log # We need to request the resources that this job will need: request_cpus = 1 request_memory = 100 MB request_disk = 1 GB # The last line of a submit file indicates how many jobs of the above # description should be queued. We'll start with one job. queue 1 More about projects \u00b6 The PATh Facility is using projects to track usage and charge the correct allocation. If you only have one project, you do not need to worry about this - the system will map your jobs automatically to that one project. However, if you have multiple ones, you will have to add an attribute to your submit file to indicate which project to charge. The attribute name is +ProjectName . For example, if your project is AmazingScience , the line in the submit file (somewhere before the queue line) should be: +ProjectName = AmazingScience Submit the job \u00b6 Submit the job using condor_submit : $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 144121. Check the job status \u00b6 The condor_q command tells the status of your jobs currently in the queue. Generally you will want to limit it to your own jobs: $ condor_q netid OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS netid ID: 1441271 12/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for netid: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended You can also get status on a specific job cluster: $ condor_q 1441271 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS netid ID: 1441271 12/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for netid: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended Note the DONE , RUN , and IDLE columns. Your job will be listed in the IDLE column if it hasn't started yet. If it's currently scheduled and running, it will appear in the RUN column. As it finishes up, it will then show in the DONE column. Once the job completes completely, it will not appear in condor_q . Let's wait for your job to finish \u2013 that is, for condor_q not to show the job in its output. A useful tool for this is condor_watch_q \u2013 it efficiently monitors the status of your jobs by monitoring their corresponding log files. Let's submit the job again, and use condor_watch_q to follow the progress of your job (the status will update at two-second intervals): $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 1441272 $ condor_watch_q ... When your job has completed, it will disappear from the list. Note : To exit out of condor_watch_q , hold down Ctrl and press C. Job history \u00b6 Once your job has finished, you can get information about its execution from the condor_history command: $ condor_history 1441272 ID OWNER SUBMITTED RUN_TIME ST COMPLETED CMD 1441272.0 netid 12/10 14:18 0+00:00:29 C 12/10 14:19 /home/netid/tutorial-quickstart/short.sh Note : You can see much more information about your job's final status using the -long option. Check the job output \u00b6 Once your job has finished, you can look at the files that HTCondor has returned to the working directory. The names of these files were specified in our submit file. If everything was successful, it should have returned: a log file from HTCondor for the job cluster: short.log an output file for each job's output: short.output an error file for each job's errors: short.error Read the output file. It should be something like this: $ cat short.output Start time: Mon Dec 10 20:18:56 UTC 2018 Job is running on node: osg-84086-0-cmswn2030.fnal.gov Job running as user: uid=12740(osg) gid=9652(osg) groups=9652(osg) Job is running in directory: /srv Working hard... Science complete! Job 2: Passing arguments to executables \u00b6 Sometimes it's useful to pass arguments to your executable from your submit file. For example, you might want to use the same job script for more than one run, varying only the parameters. You can do that by adding Arguments to your submission file. First, let's edit our existing short.sh script to accept arguments. To avoid losing our original script, we make a copy of the file under the name short_transfer.sh $ cp short.sh short_transfer.sh Now, edit the file to include the added lines below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash # short.sh: a short discovery job set -e printf \"Start time: \" ; /bin/date printf \"Job is running on node: \" ; /bin/hostname printf \"Job running as user: \" ; /usr/bin/id printf \"Job is running in directory: \" ; /bin/pwd printf \"The command line argument is: \" ; $1 printf \"Contents of $1 is \" ; cat $1 cat $1 > output.txt printf \"Working hard...\" ls -l $PWD sleep 20 echo \"Science complete!\" We need to make our new script executable just as we did before: $ chmod +x short_transfer.sh Notice that with our changes, the new script will now print out the contents of whatever file we specify in our arguments, specified by the $1 . It will also copy the contents of that file into another file called output.txt . Make a simple text file called input.txt that we can pass to our script: \"Hello World\" Once again, before submitting our job we should test it locally to ensure it runs as we expect: $ ./short_transfer.sh input.txt Start time: Tue Dec 11 10:19:12 CST 2018 Job is running on node: ap.path-facility Job running as user: uid=100279(netid) gid=1000(users) Job is running in directory: /home/netid/tutorial-quickstart The command line argument is: Contents of input.txt is \"Hello World\"Working hard...total 28 drwxrwxr-x 2 netid users 34 Oct 15 09:37 Images -rw-rw-r-- 1 netid users 13 Oct 15 09:37 input.txt drwxrwxr-x 2 netid users 114 Dec 11 09:50 log -rw-r--r-- 1 netid users 13 Dec 11 10:19 output.txt -rwxrwxr-x 1 netid users 291 Oct 15 09:37 short.sh -rwxrwxr-x 1 netid users 390 Dec 11 10:18 short_transfer.sh -rw-rw-r-- 1 netid users 806 Oct 15 09:37 tutorial01.submit -rw-rw-r-- 1 netid users 547 Dec 11 09:49 tutorial02.submit -rw-rw-r-- 1 netid users 1321 Oct 15 09:37 tutorial03.submit Science complete! Now, let's edit our submit file to properly handle these new arguments and output files and save this as tutorial02.submit # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input and output files: executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt transfer_output_files = output.txt error = job.error output = job.output log = job.log # The below are good base requirements for first testing jobs, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue one job with the above specifications. queue 1 Notice the added arguments = input.txt information. The arguements option specifies what arguments should be passed to the executable. The transfer_input_files and transfer_output_files options need to be included as well. When jobs are executed on the Open Science Pool via HTCondor, they are sent only with files that are specified. Additionally, only the specified output files are returned with the job. Any output not transferred back, with the exception of our error , output , and log files, are discarded at the end of the job. Submit the new submit file using condor_submit . Be sure to check your output files once the job completes. $ condor_submit tutorial02.submit Submitting job(s). 1 job(s) submitted to cluster 1444781. Job 3: Submitting jobs concurrently \u00b6 What do we need to do to submit several jobs simultaneously? In the first example, Condor returned three files: out, error, and log. If we want to submit several jobs, we need to track these three files for each job. An easy way to do this is to add the $(Cluster) and $(Process) macros to the HTCondor submit file. Since this can make our working directory really messy with a large number of jobs, let's tell HTCondor to put the files in a directory called log. Here's what the third submit file looks like, called tutorial03.submit : # We need the job to run our executable script, arguments and files. # Also, we'll specify unique filenames for each job by using # the job's 'cluster' value. executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt transfer_output_files = output.txt error = log/job.$(Cluster).$(Process)error output = log/job.$(Cluster).$(Process).output log = log/job.$(Cluster).$(Process).log request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Let's queue ten jobs with the above specifications queue 10 Before submitting, we also need to make sure the log directory exists. $ mkdir -p log You'll see something like the following upon submission: $ condor_submit tutorial03.submit Submitting job(s).......... 10 job(s) submitted to cluster 1444786. Look at the output files in the log directory and notice how each job received its own separate output file: $ ls ./log job.1444786.0.error job.1444786.1.error job.1444786.2.error job.1444786.3.error job.1444786.4.error job.1444786.5.error job.1444786.6.error job.1444786.7.error job.1444786.8.error job.1444786.9.error job.1444786.0.log job.1444786.1.log job.1444786.2.log job.1444786.3.log job.1444786.4.log job.1444786.5.log job.1444786.6.log job.1444786.7.log job.1444786.8.log job.1444786.9.log job.1444786.0.output job.1444786.1.output job.1444786.2.output job.1444786.3.output job.1444786.4.output job.1444786.5.output job.1444786.6.output job.1444786.7.output job.1444786.8.output job.1444786.9.output Removing jobs from HTCondor's queue \u00b6 On occasion, jobs will need to be removed for a variety of reasons (incorrect parameters, errors in submission, etc.). In these instances, the condor_rm command can be used to remove an entire job submission or just particular jobs in a submission. The condor_rm command accepts a cluster id, a job id, or username and will remove an entire cluster of jobs, a single job, or all the jobs belonging to a given user respectively. E.g. if a job submission generates 100 jobs and is assigned a cluster id of 103, then condor_rm 103.0 will remove the first job in the cluster. Likewise, condor_rm 103 will remove all the jobs in the job submission and condor_rm [username] will remove all jobs belonging to the user. The condor_rm documenation has more details on using condor_rm including ways to remove jobs based on other constraints.","title":"Quickstart-Submit Example HTCondor Jobs"},{"location":"htc_workloads/submitting_workloads/quickstart/#quickstart-submit-example-htcondor-jobs","text":"","title":"Quickstart - Submit Example HTCondor Jobs"},{"location":"htc_workloads/submitting_workloads/quickstart/#job-1-a-simple-nonparallel-job","text":"First, using a text editing program (e.g. Nano, Vim), create a file called short.sh to use as an executable for our sample job: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # short.sh: a short discovery job set -e printf \"Start time: \" ; /bin/date printf \"Job is running on node: \" ; /bin/hostname printf \"Job running as user: \" ; /usr/bin/id printf \"Job is running in directory: \" ; /bin/pwd echo echo \"Working hard...\" sleep 20 echo \"Science complete!\" Now, make the script executable. $ chmod +x short.sh","title":"Job 1: A simple, nonparallel job"},{"location":"htc_workloads/submitting_workloads/quickstart/#run-the-job-locally","text":"When possible, it is important to first test your job on your local resorces prior to submitting the job to HTCondor to run on the PATh Facility execution points. This will help identify potential errors within the code prior to queuing many HTCondor jobs. For example, we can run this executable on our local terminal by typing: $ ./short.sh Start time: Wed Aug 21 09:21:35 CDT 2013 Job is running on node: ap1.path-facility Job running as user: uid=54161(username) gid=1000(users) Job is running in directory: /home/username/quickstart Working hard... Science complete!","title":"Run the job locally"},{"location":"htc_workloads/submitting_workloads/quickstart/#create-an-htcondor-submit-file","text":"So far, so good! Let's create a simple (if verbose) HTCondor submit file by using a text editing program. Name it tutorial01.submit . # Our executable is the main program or script that we've created # to do the 'work' of a single job. executable = short.sh # We need to name the files that HTCondor should create to save the # terminal output (stdout) and error (stderr) created by our job. # Similarly, we need to name the log file where HTCondor will save # information about job execution steps. error = short.error output = short.output log = short.log # We need to request the resources that this job will need: request_cpus = 1 request_memory = 100 MB request_disk = 1 GB # The last line of a submit file indicates how many jobs of the above # description should be queued. We'll start with one job. queue 1","title":"Create an HTCondor submit file"},{"location":"htc_workloads/submitting_workloads/quickstart/#more-about-projects","text":"The PATh Facility is using projects to track usage and charge the correct allocation. If you only have one project, you do not need to worry about this - the system will map your jobs automatically to that one project. However, if you have multiple ones, you will have to add an attribute to your submit file to indicate which project to charge. The attribute name is +ProjectName . For example, if your project is AmazingScience , the line in the submit file (somewhere before the queue line) should be: +ProjectName = AmazingScience","title":"More about projects"},{"location":"htc_workloads/submitting_workloads/quickstart/#submit-the-job","text":"Submit the job using condor_submit : $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 144121.","title":"Submit the job"},{"location":"htc_workloads/submitting_workloads/quickstart/#check-the-job-status","text":"The condor_q command tells the status of your jobs currently in the queue. Generally you will want to limit it to your own jobs: $ condor_q netid OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS netid ID: 1441271 12/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for netid: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended You can also get status on a specific job cluster: $ condor_q 1441271 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS netid ID: 1441271 12/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for netid: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended Note the DONE , RUN , and IDLE columns. Your job will be listed in the IDLE column if it hasn't started yet. If it's currently scheduled and running, it will appear in the RUN column. As it finishes up, it will then show in the DONE column. Once the job completes completely, it will not appear in condor_q . Let's wait for your job to finish \u2013 that is, for condor_q not to show the job in its output. A useful tool for this is condor_watch_q \u2013 it efficiently monitors the status of your jobs by monitoring their corresponding log files. Let's submit the job again, and use condor_watch_q to follow the progress of your job (the status will update at two-second intervals): $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 1441272 $ condor_watch_q ... When your job has completed, it will disappear from the list. Note : To exit out of condor_watch_q , hold down Ctrl and press C.","title":"Check the job status"},{"location":"htc_workloads/submitting_workloads/quickstart/#job-history","text":"Once your job has finished, you can get information about its execution from the condor_history command: $ condor_history 1441272 ID OWNER SUBMITTED RUN_TIME ST COMPLETED CMD 1441272.0 netid 12/10 14:18 0+00:00:29 C 12/10 14:19 /home/netid/tutorial-quickstart/short.sh Note : You can see much more information about your job's final status using the -long option.","title":"Job history"},{"location":"htc_workloads/submitting_workloads/quickstart/#check-the-job-output","text":"Once your job has finished, you can look at the files that HTCondor has returned to the working directory. The names of these files were specified in our submit file. If everything was successful, it should have returned: a log file from HTCondor for the job cluster: short.log an output file for each job's output: short.output an error file for each job's errors: short.error Read the output file. It should be something like this: $ cat short.output Start time: Mon Dec 10 20:18:56 UTC 2018 Job is running on node: osg-84086-0-cmswn2030.fnal.gov Job running as user: uid=12740(osg) gid=9652(osg) groups=9652(osg) Job is running in directory: /srv Working hard... Science complete!","title":"Check the job output"},{"location":"htc_workloads/submitting_workloads/quickstart/#job-2-passing-arguments-to-executables","text":"Sometimes it's useful to pass arguments to your executable from your submit file. For example, you might want to use the same job script for more than one run, varying only the parameters. You can do that by adding Arguments to your submission file. First, let's edit our existing short.sh script to accept arguments. To avoid losing our original script, we make a copy of the file under the name short_transfer.sh $ cp short.sh short_transfer.sh Now, edit the file to include the added lines below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash # short.sh: a short discovery job set -e printf \"Start time: \" ; /bin/date printf \"Job is running on node: \" ; /bin/hostname printf \"Job running as user: \" ; /usr/bin/id printf \"Job is running in directory: \" ; /bin/pwd printf \"The command line argument is: \" ; $1 printf \"Contents of $1 is \" ; cat $1 cat $1 > output.txt printf \"Working hard...\" ls -l $PWD sleep 20 echo \"Science complete!\" We need to make our new script executable just as we did before: $ chmod +x short_transfer.sh Notice that with our changes, the new script will now print out the contents of whatever file we specify in our arguments, specified by the $1 . It will also copy the contents of that file into another file called output.txt . Make a simple text file called input.txt that we can pass to our script: \"Hello World\" Once again, before submitting our job we should test it locally to ensure it runs as we expect: $ ./short_transfer.sh input.txt Start time: Tue Dec 11 10:19:12 CST 2018 Job is running on node: ap.path-facility Job running as user: uid=100279(netid) gid=1000(users) Job is running in directory: /home/netid/tutorial-quickstart The command line argument is: Contents of input.txt is \"Hello World\"Working hard...total 28 drwxrwxr-x 2 netid users 34 Oct 15 09:37 Images -rw-rw-r-- 1 netid users 13 Oct 15 09:37 input.txt drwxrwxr-x 2 netid users 114 Dec 11 09:50 log -rw-r--r-- 1 netid users 13 Dec 11 10:19 output.txt -rwxrwxr-x 1 netid users 291 Oct 15 09:37 short.sh -rwxrwxr-x 1 netid users 390 Dec 11 10:18 short_transfer.sh -rw-rw-r-- 1 netid users 806 Oct 15 09:37 tutorial01.submit -rw-rw-r-- 1 netid users 547 Dec 11 09:49 tutorial02.submit -rw-rw-r-- 1 netid users 1321 Oct 15 09:37 tutorial03.submit Science complete! Now, let's edit our submit file to properly handle these new arguments and output files and save this as tutorial02.submit # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input and output files: executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt transfer_output_files = output.txt error = job.error output = job.output log = job.log # The below are good base requirements for first testing jobs, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue one job with the above specifications. queue 1 Notice the added arguments = input.txt information. The arguements option specifies what arguments should be passed to the executable. The transfer_input_files and transfer_output_files options need to be included as well. When jobs are executed on the Open Science Pool via HTCondor, they are sent only with files that are specified. Additionally, only the specified output files are returned with the job. Any output not transferred back, with the exception of our error , output , and log files, are discarded at the end of the job. Submit the new submit file using condor_submit . Be sure to check your output files once the job completes. $ condor_submit tutorial02.submit Submitting job(s). 1 job(s) submitted to cluster 1444781.","title":"Job 2: Passing arguments to executables"},{"location":"htc_workloads/submitting_workloads/quickstart/#job-3-submitting-jobs-concurrently","text":"What do we need to do to submit several jobs simultaneously? In the first example, Condor returned three files: out, error, and log. If we want to submit several jobs, we need to track these three files for each job. An easy way to do this is to add the $(Cluster) and $(Process) macros to the HTCondor submit file. Since this can make our working directory really messy with a large number of jobs, let's tell HTCondor to put the files in a directory called log. Here's what the third submit file looks like, called tutorial03.submit : # We need the job to run our executable script, arguments and files. # Also, we'll specify unique filenames for each job by using # the job's 'cluster' value. executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt transfer_output_files = output.txt error = log/job.$(Cluster).$(Process)error output = log/job.$(Cluster).$(Process).output log = log/job.$(Cluster).$(Process).log request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Let's queue ten jobs with the above specifications queue 10 Before submitting, we also need to make sure the log directory exists. $ mkdir -p log You'll see something like the following upon submission: $ condor_submit tutorial03.submit Submitting job(s).......... 10 job(s) submitted to cluster 1444786. Look at the output files in the log directory and notice how each job received its own separate output file: $ ls ./log job.1444786.0.error job.1444786.1.error job.1444786.2.error job.1444786.3.error job.1444786.4.error job.1444786.5.error job.1444786.6.error job.1444786.7.error job.1444786.8.error job.1444786.9.error job.1444786.0.log job.1444786.1.log job.1444786.2.log job.1444786.3.log job.1444786.4.log job.1444786.5.log job.1444786.6.log job.1444786.7.log job.1444786.8.log job.1444786.9.log job.1444786.0.output job.1444786.1.output job.1444786.2.output job.1444786.3.output job.1444786.4.output job.1444786.5.output job.1444786.6.output job.1444786.7.output job.1444786.8.output job.1444786.9.output","title":"Job 3: Submitting jobs concurrently"},{"location":"htc_workloads/submitting_workloads/quickstart/#removing-jobs-from-htcondors-queue","text":"On occasion, jobs will need to be removed for a variety of reasons (incorrect parameters, errors in submission, etc.). In these instances, the condor_rm command can be used to remove an entire job submission or just particular jobs in a submission. The condor_rm command accepts a cluster id, a job id, or username and will remove an entire cluster of jobs, a single job, or all the jobs belonging to a given user respectively. E.g. if a job submission generates 100 jobs and is assigned a cluster id of 103, then condor_rm 103.0 will remove the first job in the cluster. Likewise, condor_rm 103 will remove all the jobs in the job submission and condor_rm [username] will remove all jobs belonging to the user. The condor_rm documenation has more details on using condor_rm including ways to remove jobs based on other constraints.","title":"Removing jobs from HTCondor's queue"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/","text":"Easily Submit Multiple Jobs \u00b6 Overview \u00b6 HTCondor has several convenient features for streamlining high-throughput job submission. This guide provides several examples of how to leverage these features to submit multiple jobs with a single submit file. Why submit multiple jobs with a single submit file? Many options exist for streamlining your submission of multiple jobs, and this guide only covers a few examples of what is truly possible with HTCondor. Submit Multiple Jobs Using queue \u00b6 All HTCondor submit files require a queue attribute (which must also be the last line of the submit file). By default, queue will submit one job, but users can also configure the queue attribute to behave like a for loop that will submit multiple jobs, with each job varying as predefined by the user. Below are different HTCondor submit file examples for submitting batches of multiple jobs and, where applicable, how to indicate the differences between jobs in a batch with user-defined variables. Additional examples and use cases are provided further below: queue <N> - will submit N number of jobs. Examples include performing replications, where the same job must be repeated N number of times, looping through files named with numbers, and looping through a matrix where each job uses information from a specific row or column. queue <var> from <list> - will loop through a list of file names, parameters, etc. as defined in separate text file (i.e. ). This queue option is very flexible and provides users with many options for submitting multiple jobs. Organizing Jobs Into Individual Directories - another option that can be helpful in organizing multi-job submissions. These queue options are also described in the following video from HTCondor Week 2020: Submitting Multiple Jobs Using HTCondor Video What makes these queue options powerful is the ability to use user-defined variables to specify details about your jobs in the HTCondor submit file. The examples below will include the use of $(variable_name) to specify details like input file names, file locations (aka paths), etc. When selecting a variable name, users must avoid bespoke HTCondor submit file variables such as Cluster , Process , output , and input , arguments , etc. 1. Use queue N in you HTCondor submit files \u00b6 When using queue N , HTCondor will submit a total of N jobs, counting from 0 to N - 1 and each job will be assigned a unique Process id number spanning this range of values. Because the Process variable will be unique for each job, it can be used in the submit file to indicate unique filenames and filepaths for each job. The most straightforward example of using queue N is to submit N number of identical jobs. The example shown below demonstrates how to use the Cluster and Process variables to assign unique names for the HTCondor error , output , and log files for each job in the batch: # 100jobs.sub # submit 100 identical jobs log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out ... remaining submit details ... queue 100 For each job, the appropriate number, 0, 1, 2, ... 99 will replace $(Process) . $(Cluster) will be a unique number assigned to the entire 100 job batch. Each time you run condor_submit job.sub , you will be provided with the Cluster number which you will also see in the output produced by the command condor_q . If a uniquely named results file needs to be returned by each job, $(Process) and $(Cluster) can also be used as arguments , and anywhere else as needed, in the submit file: arguments = $(Cluster)_$(Process).results ... remaining submit details ... queue 100 Be sure to properly format the arguments statement according to the executable used by the job. What if my jobs are not identical? queue N may still be a great option! Additional examples for using this option include: A. Use integer numbered input files \u00b6 [user@login]$ ls *.data 0.data 1.data 2.data 3.data ... 97.data 98.data 99.data In the submit file, use: transfer_input_files = $(Process).data ... remaining submit details ... queue 100 B. Specify a row or column number for each job \u00b6 $(Process) can be used to specify a unique row or column of information in a matrix to be used by each job in the batch. The matrix needs to then be transferred with each job as input. For exmaple: transfer_input_files = matrix.csv arguments = $(Process) ... remaining submit details ... queue 100 The above exmaples assumes that your job is set up to use an argument to specify the row or column to be used by your software. C. Need N to start at 1 \u00b6 If your input files are numbered 1 - 100 instead of 0 - 99, or your matrix row starts with 1 instead of 0, you can perform basic arithmetic in the submit file: plusone = $(Process) + 1 NewProcess = $INT(plusone, %d) arguments = $(NewProcess) ... remaining submit details ... queue 100 Then use $(NewProcess) anywhere in the submit file that you would have otherwise used $(Process) . Note that there is nothing special about the names plusone and NewProcess , you can use any names you want as variables. 2. Submit multiple jobs with one or more distinct variables per job \u00b6 Think about what's different between each job that needs to be submitted. Will each job use a different input file or combination of software parameters? Do some of the jobs need more memory or disk space? Do you want to use a different software or script on a common set of input files? Using queue <var> from <list> in your submit files can make that possible! <var> can be a single user-defined variable or comma-separated list of variables to be used anywhere in the submit file. <list> is a plain text file that defines <var> for each individual job to be submitted in the batch. Suppose you need to run a program called compare_states that will run on on the following set of input files: illinois.data , nebraska.data , and wisconsin.data and each input file can analyzed as a separate job. To create a submit file that will submit all three jobs, first create a text file that lists each .data file (one file per line). This step can be performed directly on the login node, for example: [user@state-analysis]$ ls *.data > states.txt [user@state-analysis]$ cat states.txt illinois.data nebraska.data wisconsin.data Then, in the submit file, following the pattern queue <var> from <list> , replace <var> with a variable name like state and replace <list> with the list of .data files saved in states.txt : queue state from states.txt For each line in states.txt , HTCondor will submit a job and the variable $(state) can be used anywhere in the submit file to represent the name of the .data file to be used by that job. For the first job, $(state) will be illinois.data , for the second job $(state) will be nebraska.data , and so on. For example: # run_compare_states_per_state.sub transfer_input_files = $(state) arguments = $(state) executable = compare_states ... remaining submit details ... queue state from states.txt For a working example of this kind of job submission, see our Word Frequency Tutorial . Use multiple variables for each job \u00b6 Let's imagine that each state .data file contains data spanning several years and that each job needs to analyze a specific year of data. Then the states.txt file can be modified to specify this information: [user@state-analysis]$ cat states.txt illinois.data, 1995 illinois.data, 2005 nebraska.data, 1999 nebraska.data, 2005 wisconsin.data, 2000 wisconsin.data, 2015 Then modify the queue to define two <var> named state and year : queue state,year from states.txt Then the variables $(state) and $(year) can be used in the submit file: # run_compare_states_by_year.sub arguments = $(state) $(year) transfer_input_files = $(state) executable = compare_states ... remaining submit details ... queue state,year from states.txt 3. Organizing Jobs Into Individual Directories \u00b6 One way to organize jobs is to assign each job to its own directory, instead of putting files in the same directory with unique names. To continue our \\\"compare_states\\\" example, suppose there\\'s a directory for each state you want to analyze, and each of those directories has its own input file named input.data : [user@state-analysis]$ ls -F compare_states illinois/ nebraska/ wisconsin/ [user@state-analysis]$ ls -F illinois/ input.data [user@state-analysis]$ ls -F nebraska/ input.data [user@state-analysis]$ ls -F wisconsin/ input.data The HTCondor submit file attribute initialdir can be used to define a specific directory from which each job in the batch will be submitted. The default initialdir location is the directory from which the command condor_submit myjob.sub is executed. Combining queue var from list with initiadir , each line of will include the path to each state directory and initialdir set to this path for each job: #state-per-dir-job.sub initial_dir = $(state_dir) transfer_input_files = input.data executable = compare_states ... remaining submit details ... queue state_dir from state-dirs.txt Where state-dirs.txt is a list of each directory with state data: [user@state-analysis]$ cat state-dirs.txt illinois nebraska wisconsin Notice that executable = compare_states has remained unchanged in the above example. When using initialdir , only the input and output file path (including the HTCondor log, error, and output files) will be changed by initialdir . In this example, HTCondor will create a job for each directory in state-dirs.txt and use that state\\'s directory as the initialdir from which the job will be submitted. Therefore, transfer_input_files = input.data can be used without specifying the path to this input.data file. Any output generated by the job will then be returned to the initialdir location.","title":"Easily Submit Multiple Jobs"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#easily-submit-multiple-jobs","text":"","title":"Easily Submit Multiple Jobs"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#overview","text":"HTCondor has several convenient features for streamlining high-throughput job submission. This guide provides several examples of how to leverage these features to submit multiple jobs with a single submit file. Why submit multiple jobs with a single submit file? Many options exist for streamlining your submission of multiple jobs, and this guide only covers a few examples of what is truly possible with HTCondor.","title":"Overview"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#submit-multiple-jobs-using-queue","text":"All HTCondor submit files require a queue attribute (which must also be the last line of the submit file). By default, queue will submit one job, but users can also configure the queue attribute to behave like a for loop that will submit multiple jobs, with each job varying as predefined by the user. Below are different HTCondor submit file examples for submitting batches of multiple jobs and, where applicable, how to indicate the differences between jobs in a batch with user-defined variables. Additional examples and use cases are provided further below: queue <N> - will submit N number of jobs. Examples include performing replications, where the same job must be repeated N number of times, looping through files named with numbers, and looping through a matrix where each job uses information from a specific row or column. queue <var> from <list> - will loop through a list of file names, parameters, etc. as defined in separate text file (i.e. ). This queue option is very flexible and provides users with many options for submitting multiple jobs. Organizing Jobs Into Individual Directories - another option that can be helpful in organizing multi-job submissions. These queue options are also described in the following video from HTCondor Week 2020: Submitting Multiple Jobs Using HTCondor Video What makes these queue options powerful is the ability to use user-defined variables to specify details about your jobs in the HTCondor submit file. The examples below will include the use of $(variable_name) to specify details like input file names, file locations (aka paths), etc. When selecting a variable name, users must avoid bespoke HTCondor submit file variables such as Cluster , Process , output , and input , arguments , etc.","title":"Submit Multiple Jobs Using queue"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#1-use-queue-n-in-you-htcondor-submit-files","text":"When using queue N , HTCondor will submit a total of N jobs, counting from 0 to N - 1 and each job will be assigned a unique Process id number spanning this range of values. Because the Process variable will be unique for each job, it can be used in the submit file to indicate unique filenames and filepaths for each job. The most straightforward example of using queue N is to submit N number of identical jobs. The example shown below demonstrates how to use the Cluster and Process variables to assign unique names for the HTCondor error , output , and log files for each job in the batch: # 100jobs.sub # submit 100 identical jobs log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out ... remaining submit details ... queue 100 For each job, the appropriate number, 0, 1, 2, ... 99 will replace $(Process) . $(Cluster) will be a unique number assigned to the entire 100 job batch. Each time you run condor_submit job.sub , you will be provided with the Cluster number which you will also see in the output produced by the command condor_q . If a uniquely named results file needs to be returned by each job, $(Process) and $(Cluster) can also be used as arguments , and anywhere else as needed, in the submit file: arguments = $(Cluster)_$(Process).results ... remaining submit details ... queue 100 Be sure to properly format the arguments statement according to the executable used by the job. What if my jobs are not identical? queue N may still be a great option! Additional examples for using this option include:","title":"1. Use queue N in you HTCondor submit files"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#a-use-integer-numbered-input-files","text":"[user@login]$ ls *.data 0.data 1.data 2.data 3.data ... 97.data 98.data 99.data In the submit file, use: transfer_input_files = $(Process).data ... remaining submit details ... queue 100","title":"A. Use integer numbered input files"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#b-specify-a-row-or-column-number-for-each-job","text":"$(Process) can be used to specify a unique row or column of information in a matrix to be used by each job in the batch. The matrix needs to then be transferred with each job as input. For exmaple: transfer_input_files = matrix.csv arguments = $(Process) ... remaining submit details ... queue 100 The above exmaples assumes that your job is set up to use an argument to specify the row or column to be used by your software.","title":"B. Specify a row or column number for each job"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#c-need-n-to-start-at-1","text":"If your input files are numbered 1 - 100 instead of 0 - 99, or your matrix row starts with 1 instead of 0, you can perform basic arithmetic in the submit file: plusone = $(Process) + 1 NewProcess = $INT(plusone, %d) arguments = $(NewProcess) ... remaining submit details ... queue 100 Then use $(NewProcess) anywhere in the submit file that you would have otherwise used $(Process) . Note that there is nothing special about the names plusone and NewProcess , you can use any names you want as variables.","title":"C. Need N to start at 1"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#2-submit-multiple-jobs-with-one-or-more-distinct-variables-per-job","text":"Think about what's different between each job that needs to be submitted. Will each job use a different input file or combination of software parameters? Do some of the jobs need more memory or disk space? Do you want to use a different software or script on a common set of input files? Using queue <var> from <list> in your submit files can make that possible! <var> can be a single user-defined variable or comma-separated list of variables to be used anywhere in the submit file. <list> is a plain text file that defines <var> for each individual job to be submitted in the batch. Suppose you need to run a program called compare_states that will run on on the following set of input files: illinois.data , nebraska.data , and wisconsin.data and each input file can analyzed as a separate job. To create a submit file that will submit all three jobs, first create a text file that lists each .data file (one file per line). This step can be performed directly on the login node, for example: [user@state-analysis]$ ls *.data > states.txt [user@state-analysis]$ cat states.txt illinois.data nebraska.data wisconsin.data Then, in the submit file, following the pattern queue <var> from <list> , replace <var> with a variable name like state and replace <list> with the list of .data files saved in states.txt : queue state from states.txt For each line in states.txt , HTCondor will submit a job and the variable $(state) can be used anywhere in the submit file to represent the name of the .data file to be used by that job. For the first job, $(state) will be illinois.data , for the second job $(state) will be nebraska.data , and so on. For example: # run_compare_states_per_state.sub transfer_input_files = $(state) arguments = $(state) executable = compare_states ... remaining submit details ... queue state from states.txt For a working example of this kind of job submission, see our Word Frequency Tutorial .","title":"2. Submit multiple jobs with one or more distinct variables per job"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#use-multiple-variables-for-each-job","text":"Let's imagine that each state .data file contains data spanning several years and that each job needs to analyze a specific year of data. Then the states.txt file can be modified to specify this information: [user@state-analysis]$ cat states.txt illinois.data, 1995 illinois.data, 2005 nebraska.data, 1999 nebraska.data, 2005 wisconsin.data, 2000 wisconsin.data, 2015 Then modify the queue to define two <var> named state and year : queue state,year from states.txt Then the variables $(state) and $(year) can be used in the submit file: # run_compare_states_by_year.sub arguments = $(state) $(year) transfer_input_files = $(state) executable = compare_states ... remaining submit details ... queue state,year from states.txt","title":"Use multiple variables for each job"},{"location":"htc_workloads/submitting_workloads/submit-multiple-jobs/#3-organizing-jobs-into-individual-directories","text":"One way to organize jobs is to assign each job to its own directory, instead of putting files in the same directory with unique names. To continue our \\\"compare_states\\\" example, suppose there\\'s a directory for each state you want to analyze, and each of those directories has its own input file named input.data : [user@state-analysis]$ ls -F compare_states illinois/ nebraska/ wisconsin/ [user@state-analysis]$ ls -F illinois/ input.data [user@state-analysis]$ ls -F nebraska/ input.data [user@state-analysis]$ ls -F wisconsin/ input.data The HTCondor submit file attribute initialdir can be used to define a specific directory from which each job in the batch will be submitted. The default initialdir location is the directory from which the command condor_submit myjob.sub is executed. Combining queue var from list with initiadir , each line of will include the path to each state directory and initialdir set to this path for each job: #state-per-dir-job.sub initial_dir = $(state_dir) transfer_input_files = input.data executable = compare_states ... remaining submit details ... queue state_dir from state-dirs.txt Where state-dirs.txt is a list of each directory with state data: [user@state-analysis]$ cat state-dirs.txt illinois nebraska wisconsin Notice that executable = compare_states has remained unchanged in the above example. When using initialdir , only the input and output file path (including the HTCondor log, error, and output files) will be changed by initialdir . In this example, HTCondor will create a job for each directory in state-dirs.txt and use that state\\'s directory as the initialdir from which the job will be submitted. Therefore, transfer_input_files = input.data can be used without specifying the path to this input.data file. Any output generated by the job will then be returned to the initialdir location.","title":"3. Organizing Jobs Into Individual Directories"},{"location":"htc_workloads/using_data/data-transfer/","text":"Data Staging and Transfer to Jobs \u00b6 Overview \u00b6 As a distributed system, jobs in the PATh Facility can run in different physical locations, where the computers that are executing jobs don't have direct access to the files placed on the Access Point (e.g. in a /home directory on ap1.facility.path-cc.io ). In order to run on this kind of distributed system, jobs need to \"bring along\" the data, code, packages, and other files from the Access Point (where the job is submitted) to the PATh Facility execute points (where the job will run). HTCondor's file transfer tools and plugins make this possible; input and output files are specified as part of the job submission and then moved to and from the execution location. This guide describes where to place files on PATh Facility Access Points, and how to use these files within jobs. Data Spaces on the PATh Facility \u00b6 There are two spaces for placing files on the PATh Facility Access Point, and each has a corresponding transfer method for referencing files in the submit file. Location File Sizes Transfer Method Initial Quota /home/$USER Input: less than 1Gb per job file paths in transfer_input_files 50GB Output: less than 1Gb per job /path-facility/data/$USER greater than 1Gb per job OR shared files used by many jobs osdf:/// links in transfer_input_files 500GB / 250k items greater than 1Gb per job Space for Project or Public Data \u00b6 The examples above are both specific to a single user. If you need to share files with other members of your group or project, or need to make files publicly available, please contact the facilitation team to arrange a project folder: support@path-cc.io Transferring Data To/From HTCondor Jobs \u00b6 Regardless of where data is placed, jobs should only be submitted with condor_submit from /home . Transfer Smaller Job Input and Output Files to/from /home \u00b6 You should use your /home directory to stage job files where: * individual input files per job are less than 1GB per file, and if there are multiple files, they total less than 1GB * output files per job are less than 1GB per file Input Files from /home \u00b6 To transfer input files from /home , list the files by name in the transfer_input_files submit file option. You can use either absolute or relative paths to your input files. Multiple files can be specified using a comma-separated list. Some examples: Transferring multiple files from the submission directory transfer_input_files = my_data.csv, my_software.tar.gz, my_script.py Transferring a file using an absolute path is useful if a file is not in the same directory tree as your submit file: transfer_input_files = /home/username/path/to/my_software.tar.gz Output Files to /home \u00b6 By default, files created by your job will automatically be returned to your /home directory. If you would like a file to return to a diffrent subfolder within your /home directory, use HTCondor's transfer_output_remaps option. Transfer Larger Job Input and Output Files to/from /path-facility/data \u00b6 You should use your /path-facility/data directory to stage job files where: * individual input files per job are greater than 1GB per file * an input file (of any size) is used by many jobs * output files per job are greater than 1GB per file Important Note: Large files stored in /path-facility/data are cached, so it is important to use a descriptive file name (possibly using version names or dates within the file name), or a directory structure with unique names to ensure you know what version of the file you are using within your job. Behind the scenes, the files in /path-facility/data are being distributed using a network called the Open Science Data Federation (or OSDF), which is why you'll see that acronym in the commands and variables below. Input Files from /path-facility/data \u00b6 To transfer input files from /path-facility/data , use the osdf:/// plugin syntax as part of the transfer_input_files submit file option. Some examples: Transferring one file from /path-facility/data transfer_input_files = osdf:///path-facility/data/<username>/InFile.txt When using multiple files from /path-facility/data , it can be useful to use HTCondor submit file variables to make your list of files more readable: # Define a variable (example: OSDF_LOCATION) equal to the # path you would like files transferred to, and call this # variable using $(variable) OSDF_LOCATION = osdf:///path-facility/data/<username> transfer_input_files = $(OSDF_LOCATION)/InputFile.txt, $(OSDF_LOCATION)/database.sql Output Files to /path-facility/data \u00b6 If you would like a job to transfer a large file back to your /path-facility/data directory, in your HTCondor submit file, use the same osdf:/// plugin syntax as for input files, but with the HTCondor transfer_output_remaps submit file option. When transferring multiple files back to /path-facility/data in this way, you will separate the different files/remaps with a semi-colon. Some examples: Transferring one output file ( OutFile.txt ) back to /path-facility/data : transfer_output_remaps = \"OutFile.txt=osdf:///ospool/protected/<username>/OutFile.txt\" When using multiple files from /path-facility/data , it can be useful to use HTCondor submit file variables to make your list of files more readable. Also note the semi-colon separator in the list of output files. # Define a variable (example: OSDF_LOCATION) equal to the # path you would like files transferred to, and call this # variable using $(variable) OSDF_LOCATION = osdf:///path-facility/data/<username> transfer_output_remaps = \"file1.txt = $(OSDF_LOCATION)/file1.txt; file2.txt = $(OSDF_LOCATION)/file2.txt; file3.txt = $(OSDF_LOCATION)/file3.txt\" Moving Data to/from PATh Facility Access Points \u00b6 In general, common Unix tools such as rsync, scp, PuTTY, WinSCP, gFTP, etc. can be used to upload data from your computer or another server to your PATh Facility Access Point or to download files. Files should be uploaded/created and staged in /home or /path-facility for preparation to use in jobs (as described above). Check Your Quota and Available Space \u00b6 Check your /home quota \u00b6 To check your home quota and usage, run: $ quota -vs Check your /path-facility/data quota \u00b6 For now, contact the facilitation team if you are unsure what your /path-facility/data quota is. Request Quota Increase \u00b6 Contact us at support@path-cc.io if you think you need a quota increase. We have space for substantial workloads when communicated with in advance. Data Policies \u00b6 In general, users are responsible for managing data and for using appropriate mechanisms for delivering data to/from jobs. Each space for data is controlled with a quota and should be treated as temporary storage for active job execution. The PATh Facility has no routine backup of data in these locations, and users should remove old data after jobs complete. Data stored within /home and /path-facility/data is available only to your jobs, but highly sensitive data (e.g. HIPAA) should never be uploaded to PATh servces. PATh staff reserve the right to monitor and/or remove data without notice to the user if doing so is necessary for ensuring proper use or to quickly fix a performance or security issue. Additionally, users should not use PATh resources or services for long-term data storage (see above).","title":"Data Staging and Transfer to Jobs"},{"location":"htc_workloads/using_data/data-transfer/#data-staging-and-transfer-to-jobs","text":"","title":"Data Staging and Transfer to Jobs"},{"location":"htc_workloads/using_data/data-transfer/#overview","text":"As a distributed system, jobs in the PATh Facility can run in different physical locations, where the computers that are executing jobs don't have direct access to the files placed on the Access Point (e.g. in a /home directory on ap1.facility.path-cc.io ). In order to run on this kind of distributed system, jobs need to \"bring along\" the data, code, packages, and other files from the Access Point (where the job is submitted) to the PATh Facility execute points (where the job will run). HTCondor's file transfer tools and plugins make this possible; input and output files are specified as part of the job submission and then moved to and from the execution location. This guide describes where to place files on PATh Facility Access Points, and how to use these files within jobs.","title":"Overview"},{"location":"htc_workloads/using_data/data-transfer/#data-spaces-on-the-path-facility","text":"There are two spaces for placing files on the PATh Facility Access Point, and each has a corresponding transfer method for referencing files in the submit file. Location File Sizes Transfer Method Initial Quota /home/$USER Input: less than 1Gb per job file paths in transfer_input_files 50GB Output: less than 1Gb per job /path-facility/data/$USER greater than 1Gb per job OR shared files used by many jobs osdf:/// links in transfer_input_files 500GB / 250k items greater than 1Gb per job","title":"Data Spaces on the PATh Facility"},{"location":"htc_workloads/using_data/data-transfer/#space-for-project-or-public-data","text":"The examples above are both specific to a single user. If you need to share files with other members of your group or project, or need to make files publicly available, please contact the facilitation team to arrange a project folder: support@path-cc.io","title":"Space for Project or Public Data"},{"location":"htc_workloads/using_data/data-transfer/#transferring-data-tofrom-htcondor-jobs","text":"Regardless of where data is placed, jobs should only be submitted with condor_submit from /home .","title":"Transferring Data To/From HTCondor Jobs"},{"location":"htc_workloads/using_data/data-transfer/#transfer-smaller-job-input-and-output-files-tofrom-home","text":"You should use your /home directory to stage job files where: * individual input files per job are less than 1GB per file, and if there are multiple files, they total less than 1GB * output files per job are less than 1GB per file","title":"Transfer Smaller Job Input and Output Files to/from /home"},{"location":"htc_workloads/using_data/data-transfer/#input-files-from-home","text":"To transfer input files from /home , list the files by name in the transfer_input_files submit file option. You can use either absolute or relative paths to your input files. Multiple files can be specified using a comma-separated list. Some examples: Transferring multiple files from the submission directory transfer_input_files = my_data.csv, my_software.tar.gz, my_script.py Transferring a file using an absolute path is useful if a file is not in the same directory tree as your submit file: transfer_input_files = /home/username/path/to/my_software.tar.gz","title":"Input Files from /home"},{"location":"htc_workloads/using_data/data-transfer/#output-files-to-home","text":"By default, files created by your job will automatically be returned to your /home directory. If you would like a file to return to a diffrent subfolder within your /home directory, use HTCondor's transfer_output_remaps option.","title":"Output Files to /home"},{"location":"htc_workloads/using_data/data-transfer/#transfer-larger-job-input-and-output-files-tofrom-path-facilitydata","text":"You should use your /path-facility/data directory to stage job files where: * individual input files per job are greater than 1GB per file * an input file (of any size) is used by many jobs * output files per job are greater than 1GB per file Important Note: Large files stored in /path-facility/data are cached, so it is important to use a descriptive file name (possibly using version names or dates within the file name), or a directory structure with unique names to ensure you know what version of the file you are using within your job. Behind the scenes, the files in /path-facility/data are being distributed using a network called the Open Science Data Federation (or OSDF), which is why you'll see that acronym in the commands and variables below.","title":"Transfer Larger Job Input and Output Files to/from /path-facility/data"},{"location":"htc_workloads/using_data/data-transfer/#input-files-from-path-facilitydata","text":"To transfer input files from /path-facility/data , use the osdf:/// plugin syntax as part of the transfer_input_files submit file option. Some examples: Transferring one file from /path-facility/data transfer_input_files = osdf:///path-facility/data/<username>/InFile.txt When using multiple files from /path-facility/data , it can be useful to use HTCondor submit file variables to make your list of files more readable: # Define a variable (example: OSDF_LOCATION) equal to the # path you would like files transferred to, and call this # variable using $(variable) OSDF_LOCATION = osdf:///path-facility/data/<username> transfer_input_files = $(OSDF_LOCATION)/InputFile.txt, $(OSDF_LOCATION)/database.sql","title":"Input Files from /path-facility/data"},{"location":"htc_workloads/using_data/data-transfer/#output-files-to-path-facilitydata","text":"If you would like a job to transfer a large file back to your /path-facility/data directory, in your HTCondor submit file, use the same osdf:/// plugin syntax as for input files, but with the HTCondor transfer_output_remaps submit file option. When transferring multiple files back to /path-facility/data in this way, you will separate the different files/remaps with a semi-colon. Some examples: Transferring one output file ( OutFile.txt ) back to /path-facility/data : transfer_output_remaps = \"OutFile.txt=osdf:///ospool/protected/<username>/OutFile.txt\" When using multiple files from /path-facility/data , it can be useful to use HTCondor submit file variables to make your list of files more readable. Also note the semi-colon separator in the list of output files. # Define a variable (example: OSDF_LOCATION) equal to the # path you would like files transferred to, and call this # variable using $(variable) OSDF_LOCATION = osdf:///path-facility/data/<username> transfer_output_remaps = \"file1.txt = $(OSDF_LOCATION)/file1.txt; file2.txt = $(OSDF_LOCATION)/file2.txt; file3.txt = $(OSDF_LOCATION)/file3.txt\"","title":"Output Files to /path-facility/data"},{"location":"htc_workloads/using_data/data-transfer/#moving-data-tofrom-path-facility-access-points","text":"In general, common Unix tools such as rsync, scp, PuTTY, WinSCP, gFTP, etc. can be used to upload data from your computer or another server to your PATh Facility Access Point or to download files. Files should be uploaded/created and staged in /home or /path-facility for preparation to use in jobs (as described above).","title":"Moving Data to/from PATh Facility Access Points"},{"location":"htc_workloads/using_data/data-transfer/#check-your-quota-and-available-space","text":"","title":"Check Your Quota and Available Space"},{"location":"htc_workloads/using_data/data-transfer/#check-your-home-quota","text":"To check your home quota and usage, run: $ quota -vs","title":"Check your /home quota"},{"location":"htc_workloads/using_data/data-transfer/#check-your-path-facilitydata-quota","text":"For now, contact the facilitation team if you are unsure what your /path-facility/data quota is.","title":"Check your /path-facility/data quota"},{"location":"htc_workloads/using_data/data-transfer/#request-quota-increase","text":"Contact us at support@path-cc.io if you think you need a quota increase. We have space for substantial workloads when communicated with in advance.","title":"Request Quota Increase"},{"location":"htc_workloads/using_data/data-transfer/#data-policies","text":"In general, users are responsible for managing data and for using appropriate mechanisms for delivering data to/from jobs. Each space for data is controlled with a quota and should be treated as temporary storage for active job execution. The PATh Facility has no routine backup of data in these locations, and users should remove old data after jobs complete. Data stored within /home and /path-facility/data is available only to your jobs, but highly sensitive data (e.g. HIPAA) should never be uploaded to PATh servces. PATh staff reserve the right to monitor and/or remove data without notice to the user if doing so is necessary for ensuring proper use or to quickly fix a performance or security issue. Additionally, users should not use PATh resources or services for long-term data storage (see above).","title":"Data Policies"},{"location":"htc_workloads/using_software/conda-environments/","text":"Using conda to Run Python on the PATh Facility \u00b6 The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools. Overview \u00b6 When should you use Miniconda as an installation method in the PATh Facility? * Your software has specific conda-centric installation instructions. * The above is true and the software has a lot of dependencies. * You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation on the PATh Facility, create your installation environment on the Access Point and send a zipped version to your jobs. Install Miniconda and Package for Jobs \u00b6 This section of the guide goes through the steps needed to create a software installation inside Miniconda and then use a tool called conda pack to package it up for running jobs. 1. Create a Miniconda Installation \u00b6 On the Access Point, download the latest Linux miniconda installer and run it. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh Accept the license agreement and default options. At the end, you can choose whether or not to \u201cinitialize Miniconda3 by running conda init?\u201d The default is no; you would then run the eval command listed by the installer to \u201cactivate\u201d Miniconda. If you choose \u201cno\u201d you\u2019ll want to save this command so that you can reactivate the Miniconda installation when needed in the future. 2. Create a conda \"Environment\" With Your Packages \u00b6 (If you are using an environment.yml file as described later , you should instead create the environment from your environment.yml file. If you don\u2019t have an environment.yml file to work with, follow the install instructions in this section. We recommend switching to the environment.yml method of creating environments once you understand the \u201cmanual\u201d method presented here.) Make sure that you\u2019ve activated the base Miniconda environment if you haven\u2019t already. Your prompt should look like this: (base)[alice@ap1]$ To create an environment, use the conda create command and then activate the environment: (base)[alice@ap1]$ conda create -n env-name (base)[alice@ap1]$ conda activate env-name Then, run the conda install command to install the different packages and software you want to include in the installation. How this should look is often listed in the installation examples for software (e.g. Qiime2 , Pytorch ). (env-name)[alice@ap1]$ conda install pkg1 pkg2 Some Conda packages are only available via specific Conda channels which serve as repositories for hosting and managing packages. If Conda is unable to locate the requested packages using the example above, you may need to have Conda search other channels. More detail are available at https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html. Packages may also be installed via pip , but you should only do this when there is no conda package available. Once everything is installed, deactivate the environment to go back to the Miniconda \u201cbase\u201d environment. (env-name)[alice@ap1]$ conda deactivate For example, if you wanted to create an installation with pandas and matplotlib and > call the environment py-data-sci , you would use this sequence of commands: (base)[alice@ap1]$ conda create -n py-data-sci (base)[alice@ap1]$ conda activate py-data-sci (py-data-sci)[alice@ap1]$ conda install pandas matplotlib (py-data-sci)[alice@ap1]$ conda deactivate (base)[alice@ap1]$ More About Miniconda \u00b6 See the official conda documentation for more information on creating and managing environments with conda . 3. Create Software Package \u00b6 Make sure that your job\u2019s Miniconda environment is created, but deactivated, so that you\u2019re in the \u201cbase\u201d Miniconda environment: (base)[alice@ap1]$ Then, run this command to install the conda pack tool: conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, use conda pack to create a zipped tar.gz file of your environment (substitute the name of your conda environment where you see env-name ), set the proper permissions for this file using chmod , and check the size of the final tarball: conda pack -n env-name chmod 644 env-name.tar.gz ls -sh env-name.tar.gz When this step finishes, you should see a file in your current directory named env-name.tar.gz . 4. Check Size of Conda Environment Tar Archive \u00b6 The tar archive, env-name.tar.gz , created in the previous step will be used as input for subsequent job submission. As with all job input files, you should check the size of this Conda environment file. If >100MB in size, you should NOT transfer the tar ball using transfer_input_files from your home directory . Instead, you should plan to use the /path-facility/data/$USER folder, and a stash:/// link. 5. Create a Job Executable \u00b6 The job will need to go through a few steps to use this \u201cpacked\u201d conda environment; first, setting the PATH , then unzipping the environment, then activating it, and finally running whatever program you like. The script below is an example of what is needed (customize as indicated to match your choices above). #!/bin/bash # have job exit if any command returns with non-zero exit status (aka failure) set -e # replace env-name on the right hand side of this line with the name of your conda environment ENVNAME=env-name # if you need the environment directory to be named something other than the environment name, change this line ENVDIR=$ENVNAME # these lines handle setting up the environment; you shouldn't have to modify them export PATH mkdir $ENVDIR tar -xzf $ENVNAME.tar.gz -C $ENVDIR . $ENVDIR/bin/activate # modify this line to run your desired Python script and any other work you need to do python3 hello.py 6. Submit Jobs \u00b6 In your submit file, make sure to have the following: Your executable should be the the bash script you created in step 5 . Remember to transfer your Python script and the environment tar.gz file to the job. If the tar.gz file is larger than 100MB, please use the stash:/// file delivery mechanism as described above. Specifying Exact Dependency Versions \u00b6 An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. This file can be re-used by a different conda command to recreate that exact environment on another computer. To create an environment.yml file from your currently-activated environment, run conda env export > environment.yml This environment.yml will pin the exact version of every dependency in your environment. This can sometimes be problematic if you are moving between platforms because a package version may not be available on some other platform, causing an \u201cunsatisfiable dependency\u201d or \u201cinconsistent environment\u201d error. A much less strict pinning is conda env export --from-history > environment.yml which only lists packages that you installed manually, and does not pin their versions unless you yourself pinned them during installation . If you need an intermediate solution, it is also possible to manually edit environment.yml files; see the conda environment documentation for more details about the format and what is possible. In general, exact environment specifications are simply not guaranteed to be transferable between platforms (e.g., between Windows and Linux). We strongly recommend using the strictest possible pinning available to you . To create an environment from an environment.yml file, run conda env create -f environment.yml By default, the name of the environment will be whatever the name of the source environment was; you can change the name by adding a -n \\<name> option to the conda env create command. If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. If you are developing software on your local computer for eventual use on the Open Science pool, your workflow might look like this: Set up a conda environment for local development and install packages as desired (e.g., conda create -n science; conda activate science; conda install numpy ). Once you are ready to run on the Open Science pool, create an environment.yml file from your local environment (e.g., conda env export > environment.yml ). Move your environment.yml file from your local computer to the submit machine and create an environment from it (e.g., conda env create -f environment.yml ), then pack it for use in your jobs, as per Create Software Package . More information on conda environments can be found in their documentation .","title":"Using conda to Run Python on the PATh Facility"},{"location":"htc_workloads/using_software/conda-environments/#using-conda-to-run-python-on-the-path-facility","text":"The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools.","title":"Using conda to Run Python on the PATh Facility"},{"location":"htc_workloads/using_software/conda-environments/#overview","text":"When should you use Miniconda as an installation method in the PATh Facility? * Your software has specific conda-centric installation instructions. * The above is true and the software has a lot of dependencies. * You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation on the PATh Facility, create your installation environment on the Access Point and send a zipped version to your jobs.","title":"Overview"},{"location":"htc_workloads/using_software/conda-environments/#install-miniconda-and-package-for-jobs","text":"This section of the guide goes through the steps needed to create a software installation inside Miniconda and then use a tool called conda pack to package it up for running jobs.","title":"Install Miniconda and Package for Jobs"},{"location":"htc_workloads/using_software/conda-environments/#1-create-a-miniconda-installation","text":"On the Access Point, download the latest Linux miniconda installer and run it. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh Accept the license agreement and default options. At the end, you can choose whether or not to \u201cinitialize Miniconda3 by running conda init?\u201d The default is no; you would then run the eval command listed by the installer to \u201cactivate\u201d Miniconda. If you choose \u201cno\u201d you\u2019ll want to save this command so that you can reactivate the Miniconda installation when needed in the future.","title":"1. Create a Miniconda Installation"},{"location":"htc_workloads/using_software/conda-environments/#2-create-a-conda-environment-with-your-packages","text":"(If you are using an environment.yml file as described later , you should instead create the environment from your environment.yml file. If you don\u2019t have an environment.yml file to work with, follow the install instructions in this section. We recommend switching to the environment.yml method of creating environments once you understand the \u201cmanual\u201d method presented here.) Make sure that you\u2019ve activated the base Miniconda environment if you haven\u2019t already. Your prompt should look like this: (base)[alice@ap1]$ To create an environment, use the conda create command and then activate the environment: (base)[alice@ap1]$ conda create -n env-name (base)[alice@ap1]$ conda activate env-name Then, run the conda install command to install the different packages and software you want to include in the installation. How this should look is often listed in the installation examples for software (e.g. Qiime2 , Pytorch ). (env-name)[alice@ap1]$ conda install pkg1 pkg2 Some Conda packages are only available via specific Conda channels which serve as repositories for hosting and managing packages. If Conda is unable to locate the requested packages using the example above, you may need to have Conda search other channels. More detail are available at https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html. Packages may also be installed via pip , but you should only do this when there is no conda package available. Once everything is installed, deactivate the environment to go back to the Miniconda \u201cbase\u201d environment. (env-name)[alice@ap1]$ conda deactivate For example, if you wanted to create an installation with pandas and matplotlib and > call the environment py-data-sci , you would use this sequence of commands: (base)[alice@ap1]$ conda create -n py-data-sci (base)[alice@ap1]$ conda activate py-data-sci (py-data-sci)[alice@ap1]$ conda install pandas matplotlib (py-data-sci)[alice@ap1]$ conda deactivate (base)[alice@ap1]$","title":"2. Create a conda \"Environment\" With Your Packages"},{"location":"htc_workloads/using_software/conda-environments/#more-about-miniconda","text":"See the official conda documentation for more information on creating and managing environments with conda .","title":"More About Miniconda"},{"location":"htc_workloads/using_software/conda-environments/#3-create-software-package","text":"Make sure that your job\u2019s Miniconda environment is created, but deactivated, so that you\u2019re in the \u201cbase\u201d Miniconda environment: (base)[alice@ap1]$ Then, run this command to install the conda pack tool: conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, use conda pack to create a zipped tar.gz file of your environment (substitute the name of your conda environment where you see env-name ), set the proper permissions for this file using chmod , and check the size of the final tarball: conda pack -n env-name chmod 644 env-name.tar.gz ls -sh env-name.tar.gz When this step finishes, you should see a file in your current directory named env-name.tar.gz .","title":"3. Create Software Package"},{"location":"htc_workloads/using_software/conda-environments/#4-check-size-of-conda-environment-tar-archive","text":"The tar archive, env-name.tar.gz , created in the previous step will be used as input for subsequent job submission. As with all job input files, you should check the size of this Conda environment file. If >100MB in size, you should NOT transfer the tar ball using transfer_input_files from your home directory . Instead, you should plan to use the /path-facility/data/$USER folder, and a stash:/// link.","title":"4. Check Size of Conda Environment Tar Archive"},{"location":"htc_workloads/using_software/conda-environments/#5-create-a-job-executable","text":"The job will need to go through a few steps to use this \u201cpacked\u201d conda environment; first, setting the PATH , then unzipping the environment, then activating it, and finally running whatever program you like. The script below is an example of what is needed (customize as indicated to match your choices above). #!/bin/bash # have job exit if any command returns with non-zero exit status (aka failure) set -e # replace env-name on the right hand side of this line with the name of your conda environment ENVNAME=env-name # if you need the environment directory to be named something other than the environment name, change this line ENVDIR=$ENVNAME # these lines handle setting up the environment; you shouldn't have to modify them export PATH mkdir $ENVDIR tar -xzf $ENVNAME.tar.gz -C $ENVDIR . $ENVDIR/bin/activate # modify this line to run your desired Python script and any other work you need to do python3 hello.py","title":"5. Create a Job Executable"},{"location":"htc_workloads/using_software/conda-environments/#6-submit-jobs","text":"In your submit file, make sure to have the following: Your executable should be the the bash script you created in step 5 . Remember to transfer your Python script and the environment tar.gz file to the job. If the tar.gz file is larger than 100MB, please use the stash:/// file delivery mechanism as described above.","title":"6. Submit Jobs"},{"location":"htc_workloads/using_software/conda-environments/#specifying-exact-dependency-versions","text":"An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. This file can be re-used by a different conda command to recreate that exact environment on another computer. To create an environment.yml file from your currently-activated environment, run conda env export > environment.yml This environment.yml will pin the exact version of every dependency in your environment. This can sometimes be problematic if you are moving between platforms because a package version may not be available on some other platform, causing an \u201cunsatisfiable dependency\u201d or \u201cinconsistent environment\u201d error. A much less strict pinning is conda env export --from-history > environment.yml which only lists packages that you installed manually, and does not pin their versions unless you yourself pinned them during installation . If you need an intermediate solution, it is also possible to manually edit environment.yml files; see the conda environment documentation for more details about the format and what is possible. In general, exact environment specifications are simply not guaranteed to be transferable between platforms (e.g., between Windows and Linux). We strongly recommend using the strictest possible pinning available to you . To create an environment from an environment.yml file, run conda env create -f environment.yml By default, the name of the environment will be whatever the name of the source environment was; you can change the name by adding a -n \\<name> option to the conda env create command. If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. If you are developing software on your local computer for eventual use on the Open Science pool, your workflow might look like this: Set up a conda environment for local development and install packages as desired (e.g., conda create -n science; conda activate science; conda install numpy ). Once you are ready to run on the Open Science pool, create an environment.yml file from your local environment (e.g., conda env export > environment.yml ). Move your environment.yml file from your local computer to the submit machine and create an environment from it (e.g., conda env create -f environment.yml ), then pack it for use in your jobs, as per Create Software Package . More information on conda environments can be found in their documentation .","title":"Specifying Exact Dependency Versions"},{"location":"overview/account_setup/generate-add-sshkey/","text":"Generate SSH Keys and Activate Your PATh Facility Login \u00b6 Overview \u00b6 Logging into a PATh Facility Access Point requires authenticating your credientials using one of two options: web authentication or SSH key pair authentication . This guide is for users who would like to authenticate using SSH key pairs. To use this approach, you need to follow a two-step process to associate your SSH key to your PATh Facility account. Generate an SSH key pair. Add your public key to the PATh Access Point by uploading it to your user profile on the PATh Facility registration website . After completing the process, you can log in to the PATh Access Point ( ap1.facility.path-cc.io ) from a local computer (your laptop or desktop) using either ssh or an ssh program like Putty -- see below for more details on logging in. NOTE: Please do not edit the authorized keys file on the Access Point. Step 1: Generate SSH Keys \u00b6 We will discuss how to generate a SSH key pair for two cases: \"Unix\" systems (Linux, Mac) and certain, latest versions of Windows Older Windows systems Please note: The key pair consist of a private key and a public key. You will upload the public key PATh Facility registration website , but you also need to keep a copy of the private key to log in! You should keep the private key on machines that you have direct access to, i.e. your local computer (your laptop or desktop). Unix-based operating system (Linux/Mac) or latest Windows 10 versions \u00b6 We will create a key in the .ssh directory of your computer. Open a terminal on your local computer and run the following commands: mkdir ~/.ssh chmod 700 ~/.ssh ssh-keygen -t rsa For the newer OS versions the .ssh directory is already created and the first command is redundant. The last command will produce a prompt similar to Generating public/private rsa key pair. Enter file in which to save the key (/home/<local_user_name>/.ssh/id_rsa): Unless you want to change the location of the key, continue by pressing enter. Now you will be asked for a passphrase. Enter a passphrase that you will be able to remember and which is secure: Enter passphrase (empty for no passphrase): Enter same passphrase again: When everything has successfully completed, the output should resemble the following: Your identification has been saved in /home/<local_user_name>/.ssh/id_rsa. Your public key has been saved in /home/<local_user_name>/.ssh/id_rsa.pub. The key fingerprint is: ... See Step 2 below to learn how to upload the contents of the .pub file (~/.ssh/id_rsa.pub) to the PATh Facility registration website. Windows, using Putty to log in \u00b6 If you can connect using the ssh command within the Command Prompt (Windows 10 build version 1803 and later), please follow the Mac/Linux directions above. If not, continue with the directions below. Open the PuTTYgen program. You can download PuttyGen here: PuttyGen Download Page , scroll down until you see the puttygen.exe file. For Type of key to generate, select RSA or SSH-2 RSA. Click the \"Generate\" button. Move your mouse in the area below the progress bar. When the progress bar is full, PuTTYgen generates your key pair. Type a passphrase in the \"Key passphrase\" field. Type the same passphrase in the \"Confirm passphrase\" field. You can use a key without a passphrase, but this is not recommended. Click the \"Save private key\" and \"Save public key\" buttons to save both keys. You must save both keys. You will need them to successfully login from your machine. In Step 2 below, you will upload your public key to the PATh Facility registration website . The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . Step 2: Add the public SSH key to login node \u00b6 To add your public key to the PATh Facility registration website : Go to the PATh Facility registration website and sign in with the institutional identity you used when requesting an account. Click your name at the top right. In the dropdown box, click \"My Profile (OSG)\" button. On the right hand side of your profile, click \"Authenticators\" link. On the authenticators page, click the \"Manage\" button. On the new SSH Keys page, click \"Add SSH Key\" and browse your computer to upload your public SSH key. The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . Mac Operating Systems : When you click \"browse\", your Mac Finder window will appear. At the top of the Finder window, open the drop-down box and select \"Macintosh HD\". Then navigate to \"Users\" and select your username. Using your keyboard, type Command + Shift + . to reveal hidden files and directories. Open the .ssh directory, and select the id_rsa.pub file. Click the \"Upload\" box to upload your public SSH key. Windows Operating Systems : When you click \"browse\", an Explorer window will appear. Navigate to the directory where you saved your public key file. Select it and click the \"Upload\" box to upload your public SSH key. Linux Operating Systems : When you click \"browse\", the window that appears may vary depending on your Linux operating system as well as your browser. You will want to navigate to your id_rsa.pub file, which is typically found in the path of Users/[your user name]/.ssh . To show hidden files/directories on a machine running Ubuntu or Debian, use the keyboard shortcut ctrl + h . If you have trouble finding your id_ssh.pub file for your specific Linux environment, there are many resources online. Additionally, our Research Computing Facilitators are here to help you should you need it. Once you have added your public key, click \"Update\". You should be able to login within a few hours. Can I Use Multiple Keys? \u00b6 Yes! If you want to log into the PATh Facility from multiple computers, you can do so by generating a keypair on each computer you want to use, and then adding the public key to your PATh Facility profile. Logging In \u00b6 After following the steps above to upload your key and it's been a few hours, you should be able to log in to begin using PATh Facility compute resources. For Mac, Linux, or newer versions of Windows \u00b6 Open a terminal and type in: ssh <your_PATh_Facility_username>@ap1.facility.path-cc.io It will ask for the passphrase for your ssh key (if you set one) and then you should be logged in. For older versions of Windows \u00b6 On older versions of Windows, you can use the Putty program to log in. Open the PutTTY program. If necessary, you can download PuTTY from the website here PuTTY download page . Type in ap1.facility.path-cc.io as the as the hostname address. Leave Port 22 as the default Port number. In the left hand menu, click the \"+\" next to \"SSH\" to expand the menu. Click \"Auth\" in the \"SSH\" menu. Click \"Browse\" and specify the private key file you saved in Step 1. Return to \"Session\". a. Name your session b. Save session for future use Click \"Open\" to launch shell. Provide your ssh-key passphrase (created above in PuTTYgen) when prompted to do so. Get Help \u00b6 For questions regarding logging in or creating an account, contact us at support@path-cc.io .","title":"Log in with SSH Keys"},{"location":"overview/account_setup/generate-add-sshkey/#generate-ssh-keys-and-activate-your-path-facility-login","text":"","title":"Generate SSH Keys and Activate Your PATh Facility Login"},{"location":"overview/account_setup/generate-add-sshkey/#overview","text":"Logging into a PATh Facility Access Point requires authenticating your credientials using one of two options: web authentication or SSH key pair authentication . This guide is for users who would like to authenticate using SSH key pairs. To use this approach, you need to follow a two-step process to associate your SSH key to your PATh Facility account. Generate an SSH key pair. Add your public key to the PATh Access Point by uploading it to your user profile on the PATh Facility registration website . After completing the process, you can log in to the PATh Access Point ( ap1.facility.path-cc.io ) from a local computer (your laptop or desktop) using either ssh or an ssh program like Putty -- see below for more details on logging in. NOTE: Please do not edit the authorized keys file on the Access Point.","title":"Overview"},{"location":"overview/account_setup/generate-add-sshkey/#step-1-generate-ssh-keys","text":"We will discuss how to generate a SSH key pair for two cases: \"Unix\" systems (Linux, Mac) and certain, latest versions of Windows Older Windows systems Please note: The key pair consist of a private key and a public key. You will upload the public key PATh Facility registration website , but you also need to keep a copy of the private key to log in! You should keep the private key on machines that you have direct access to, i.e. your local computer (your laptop or desktop).","title":"Step 1: Generate SSH Keys"},{"location":"overview/account_setup/generate-add-sshkey/#unix-based-operating-system-linuxmac-or-latest-windows-10-versions","text":"We will create a key in the .ssh directory of your computer. Open a terminal on your local computer and run the following commands: mkdir ~/.ssh chmod 700 ~/.ssh ssh-keygen -t rsa For the newer OS versions the .ssh directory is already created and the first command is redundant. The last command will produce a prompt similar to Generating public/private rsa key pair. Enter file in which to save the key (/home/<local_user_name>/.ssh/id_rsa): Unless you want to change the location of the key, continue by pressing enter. Now you will be asked for a passphrase. Enter a passphrase that you will be able to remember and which is secure: Enter passphrase (empty for no passphrase): Enter same passphrase again: When everything has successfully completed, the output should resemble the following: Your identification has been saved in /home/<local_user_name>/.ssh/id_rsa. Your public key has been saved in /home/<local_user_name>/.ssh/id_rsa.pub. The key fingerprint is: ... See Step 2 below to learn how to upload the contents of the .pub file (~/.ssh/id_rsa.pub) to the PATh Facility registration website.","title":"Unix-based operating system (Linux/Mac) or latest Windows 10 versions"},{"location":"overview/account_setup/generate-add-sshkey/#windows-using-putty-to-log-in","text":"If you can connect using the ssh command within the Command Prompt (Windows 10 build version 1803 and later), please follow the Mac/Linux directions above. If not, continue with the directions below. Open the PuTTYgen program. You can download PuttyGen here: PuttyGen Download Page , scroll down until you see the puttygen.exe file. For Type of key to generate, select RSA or SSH-2 RSA. Click the \"Generate\" button. Move your mouse in the area below the progress bar. When the progress bar is full, PuTTYgen generates your key pair. Type a passphrase in the \"Key passphrase\" field. Type the same passphrase in the \"Confirm passphrase\" field. You can use a key without a passphrase, but this is not recommended. Click the \"Save private key\" and \"Save public key\" buttons to save both keys. You must save both keys. You will need them to successfully login from your machine. In Step 2 below, you will upload your public key to the PATh Facility registration website . The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host .","title":"Windows, using Putty to log in"},{"location":"overview/account_setup/generate-add-sshkey/#step-2-add-the-public-ssh-key-to-login-node","text":"To add your public key to the PATh Facility registration website : Go to the PATh Facility registration website and sign in with the institutional identity you used when requesting an account. Click your name at the top right. In the dropdown box, click \"My Profile (OSG)\" button. On the right hand side of your profile, click \"Authenticators\" link. On the authenticators page, click the \"Manage\" button. On the new SSH Keys page, click \"Add SSH Key\" and browse your computer to upload your public SSH key. The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . Mac Operating Systems : When you click \"browse\", your Mac Finder window will appear. At the top of the Finder window, open the drop-down box and select \"Macintosh HD\". Then navigate to \"Users\" and select your username. Using your keyboard, type Command + Shift + . to reveal hidden files and directories. Open the .ssh directory, and select the id_rsa.pub file. Click the \"Upload\" box to upload your public SSH key. Windows Operating Systems : When you click \"browse\", an Explorer window will appear. Navigate to the directory where you saved your public key file. Select it and click the \"Upload\" box to upload your public SSH key. Linux Operating Systems : When you click \"browse\", the window that appears may vary depending on your Linux operating system as well as your browser. You will want to navigate to your id_rsa.pub file, which is typically found in the path of Users/[your user name]/.ssh . To show hidden files/directories on a machine running Ubuntu or Debian, use the keyboard shortcut ctrl + h . If you have trouble finding your id_ssh.pub file for your specific Linux environment, there are many resources online. Additionally, our Research Computing Facilitators are here to help you should you need it. Once you have added your public key, click \"Update\". You should be able to login within a few hours.","title":"Step 2: Add the public SSH key to login node"},{"location":"overview/account_setup/generate-add-sshkey/#can-i-use-multiple-keys","text":"Yes! If you want to log into the PATh Facility from multiple computers, you can do so by generating a keypair on each computer you want to use, and then adding the public key to your PATh Facility profile.","title":"Can I Use Multiple Keys?"},{"location":"overview/account_setup/generate-add-sshkey/#logging-in","text":"After following the steps above to upload your key and it's been a few hours, you should be able to log in to begin using PATh Facility compute resources.","title":"Logging In"},{"location":"overview/account_setup/generate-add-sshkey/#for-mac-linux-or-newer-versions-of-windows","text":"Open a terminal and type in: ssh <your_PATh_Facility_username>@ap1.facility.path-cc.io It will ask for the passphrase for your ssh key (if you set one) and then you should be logged in.","title":"For Mac, Linux, or newer versions of Windows"},{"location":"overview/account_setup/generate-add-sshkey/#for-older-versions-of-windows","text":"On older versions of Windows, you can use the Putty program to log in. Open the PutTTY program. If necessary, you can download PuTTY from the website here PuTTY download page . Type in ap1.facility.path-cc.io as the as the hostname address. Leave Port 22 as the default Port number. In the left hand menu, click the \"+\" next to \"SSH\" to expand the menu. Click \"Auth\" in the \"SSH\" menu. Click \"Browse\" and specify the private key file you saved in Step 1. Return to \"Session\". a. Name your session b. Save session for future use Click \"Open\" to launch shell. Provide your ssh-key passphrase (created above in PuTTYgen) when prompted to do so.","title":"For older versions of Windows"},{"location":"overview/account_setup/generate-add-sshkey/#get-help","text":"For questions regarding logging in or creating an account, contact us at support@path-cc.io .","title":"Get Help"},{"location":"overview/account_setup/getting-started/","text":"Getting Started on the PATh Facility \u00b6 The PATh Facility is part of a pilot project funded by the NSF Office of Advanced Cyberinfrastructure. NSF-funded researchers can apply to request credits for the PATh facility. The facility aims to demonstrate the value of dedicated, distributed resources to the NSF Science and Engineering community. Composed of current gen hardware, consisting of 30,000 cores and 36 A100 GPUs the PATh facility is built to run your high throughput computing workloads. Managed by the HTCondor Suite's Access Point the PATh facility is managed to make running these workloads easy. To get started on the PATh Facility: Confirm that you meet PATh Facility criteria Request an account Apply for credits (PATh Staff can provide immediate startup allocations for basic workflow prototyping.) These steps are described below: Meet PATh Facility Criteria \u00b6 Groups are eligible to use the PATh Facility if: they have an accepted or active NSF award with one of the affiliated programs below they have an NSF proposal in preparation or pending review with one of the programs listed below NSF Programs: Computational and Data-Enabled Science and Engineering Cyberinfrastructure for Sustained Scientific Innovation Innovation: Bioinformatics Neural Systems Collaborative Research in Computational Neuroscience Astronomy and Astrophysics Research Grants Chemical Theory, Models, and Computational Methods Condensed Matter and Materials Theory Atomic, Molecular and Optical Physics \u2012 Theory Nuclear Physics \u2012 Theory Geoinformatics (GI) Geophysics (PH) Arctic Research Opportunities Antarctic Research Request an Account \u00b6 To request an account on the PATh Facility, fill out the request form here: PATh User Application After filling out this form, a member of our Facilitation team will be in touch to arrange a short consultation and provide additional information about activating your account. Request Credits \u00b6 All compute time on the PATh Facility is charged using a credit system. If you have not already applied for a credit allocation at the time of requesting an account, PATh staff can provide a small startup allocation for initial testing. After start up testing, credits are allocated from the NSF via the relevant program officer. See our Request PATh Facility Credits guide for detailed information and templates on submitting a credit request. If you have already applied for a credit allocation at the time of requesting an account, let us know that information and we will add credits to your project account. Reach Out \u00b6 If you're not sure where to start or are not sure if the PATh Facility is for you, we recommend either filling out the User Application form linked above or emailing PATh support staff at support@path-cc.io with any questions.","title":"Getting Started"},{"location":"overview/account_setup/getting-started/#getting-started-on-the-path-facility","text":"The PATh Facility is part of a pilot project funded by the NSF Office of Advanced Cyberinfrastructure. NSF-funded researchers can apply to request credits for the PATh facility. The facility aims to demonstrate the value of dedicated, distributed resources to the NSF Science and Engineering community. Composed of current gen hardware, consisting of 30,000 cores and 36 A100 GPUs the PATh facility is built to run your high throughput computing workloads. Managed by the HTCondor Suite's Access Point the PATh facility is managed to make running these workloads easy. To get started on the PATh Facility: Confirm that you meet PATh Facility criteria Request an account Apply for credits (PATh Staff can provide immediate startup allocations for basic workflow prototyping.) These steps are described below:","title":"Getting Started on the PATh Facility"},{"location":"overview/account_setup/getting-started/#meet-path-facility-criteria","text":"Groups are eligible to use the PATh Facility if: they have an accepted or active NSF award with one of the affiliated programs below they have an NSF proposal in preparation or pending review with one of the programs listed below NSF Programs: Computational and Data-Enabled Science and Engineering Cyberinfrastructure for Sustained Scientific Innovation Innovation: Bioinformatics Neural Systems Collaborative Research in Computational Neuroscience Astronomy and Astrophysics Research Grants Chemical Theory, Models, and Computational Methods Condensed Matter and Materials Theory Atomic, Molecular and Optical Physics \u2012 Theory Nuclear Physics \u2012 Theory Geoinformatics (GI) Geophysics (PH) Arctic Research Opportunities Antarctic Research","title":"Meet PATh Facility Criteria"},{"location":"overview/account_setup/getting-started/#request-an-account","text":"To request an account on the PATh Facility, fill out the request form here: PATh User Application After filling out this form, a member of our Facilitation team will be in touch to arrange a short consultation and provide additional information about activating your account.","title":"Request an Account"},{"location":"overview/account_setup/getting-started/#request-credits","text":"All compute time on the PATh Facility is charged using a credit system. If you have not already applied for a credit allocation at the time of requesting an account, PATh staff can provide a small startup allocation for initial testing. After start up testing, credits are allocated from the NSF via the relevant program officer. See our Request PATh Facility Credits guide for detailed information and templates on submitting a credit request. If you have already applied for a credit allocation at the time of requesting an account, let us know that information and we will add credits to your project account.","title":"Request Credits"},{"location":"overview/account_setup/getting-started/#reach-out","text":"If you're not sure where to start or are not sure if the PATh Facility is for you, we recommend either filling out the User Application form linked above or emailing PATh support staff at support@path-cc.io with any questions.","title":"Reach Out"},{"location":"overview/account_setup/registration/","text":"Registration and Login for the PATh Facility \u00b6 This guide is for users who have already contacted PATh Staff about accessing the PATh Facility and have been directed to these instructions. If you have not been sent this information by PATh staff, please contact us at support@path-cc.io or fill out the PATh User Application . To join and use the PATh Facility Access Point (ap1.facility.path-cc.io), you will go through the following steps: Apply for a PATh Facility Access Point account Have your account approved by an PATh Team member Log in to ap1.facility.path-cc.io Apply for a PATh Facility Access Point Account \u00b6 To register with the PATh facility, submit an application using the following steps: Go to the account registration page . You will be redirected to the CILogon sign in page. Select your institution and use your institutional credentials to login. If you have issues signing in using your institutional credentials, contact us at support@path-cc.io . Once you sign in, you will be redirected to the \"PATh Facility User Enrollment for New Users\" page. Click \"Begin\" and enter your name, and email address in the following page. In many cases, this information will be automatically populated. If desired, it is possible to manually edit any information automatically filled in. Once you have entered your information, click \"SUBMIT\". After submitting your application, you will receive an email from registry@cilogon.org to verify your email address. Click the link listed in the email to be redirected to a page confirm your invitation details. Click the \"ACCEPT\" button to complete this step. Account Approval by a Research Computing Facilitator \u00b6 If a meeting has not already been scheduled with a Research Computing Facilitator, one of the facilitation team will contact you about arranging a short consultation. Following the meeting, the Facilitator will approve your account and add your profile to any relevant PATh Facility \u2018project\u2019 names. Once your account is ready, the Facilitator will email you with your account details including the username you will use to log in to the ap1.facility.path-cc.io access point. Login \u00b6 Once your account has been added to a PATh access point, you will be able to log in using a terminal or SSH program. Logging in requires authenticating your credientials using one of two options: web authentication or SSH key pair authentication . Additional information on this process will be provided during and/or following your meeting with a Research Computing Facilitator. Option 1: Login via Web Authentication \u00b6 Logging in via web authentication requires no preparatory steps beyond having access to an internet browser. To authenticate using this approach: Open a terminal and type ssh username@ap1.facility.path-cc.io , being sure to replace username with your PATh access point username. Upon hitting enter, the following text should appear with a unique, but similar, URL: Authenticate at ----------------- https://cilogon.org/device/?user_code=FF4-ZX6-9LK ----------------- Type 'Enter' when you authenticate. Copy the https:// link, paste it into a web browser, and hit enter. You will be redirected to a new page where you will be prompted to login using your institutional credentials. Once you have done so, a new page will appear with the following text: \"You have successfully approved the user code. Please return to your device for further instructions.\" Return to your terminal, and type 'Enter' to complete the login process. Option 2: Login via SSH Key Pair Authentication \u00b6 It is also possible to authenticate using an SSH key pair, if you prefer. Logging in using SSH keys does not require access to an internet browser to login into the PATh access point, ap1.facility.path-cc.io . Full details across multiple configurations are explained in our SSH Authentication guide . Get Help \u00b6 For questions regarding logging in or creating an account, contact us at support@path-cc.io .","title":"Register for a PATh Facility Account"},{"location":"overview/account_setup/registration/#registration-and-login-for-the-path-facility","text":"This guide is for users who have already contacted PATh Staff about accessing the PATh Facility and have been directed to these instructions. If you have not been sent this information by PATh staff, please contact us at support@path-cc.io or fill out the PATh User Application . To join and use the PATh Facility Access Point (ap1.facility.path-cc.io), you will go through the following steps: Apply for a PATh Facility Access Point account Have your account approved by an PATh Team member Log in to ap1.facility.path-cc.io","title":"Registration and Login for the PATh Facility"},{"location":"overview/account_setup/registration/#apply-for-a-path-facility-access-point-account","text":"To register with the PATh facility, submit an application using the following steps: Go to the account registration page . You will be redirected to the CILogon sign in page. Select your institution and use your institutional credentials to login. If you have issues signing in using your institutional credentials, contact us at support@path-cc.io . Once you sign in, you will be redirected to the \"PATh Facility User Enrollment for New Users\" page. Click \"Begin\" and enter your name, and email address in the following page. In many cases, this information will be automatically populated. If desired, it is possible to manually edit any information automatically filled in. Once you have entered your information, click \"SUBMIT\". After submitting your application, you will receive an email from registry@cilogon.org to verify your email address. Click the link listed in the email to be redirected to a page confirm your invitation details. Click the \"ACCEPT\" button to complete this step.","title":"Apply for a PATh Facility Access Point Account"},{"location":"overview/account_setup/registration/#account-approval-by-a-research-computing-facilitator","text":"If a meeting has not already been scheduled with a Research Computing Facilitator, one of the facilitation team will contact you about arranging a short consultation. Following the meeting, the Facilitator will approve your account and add your profile to any relevant PATh Facility \u2018project\u2019 names. Once your account is ready, the Facilitator will email you with your account details including the username you will use to log in to the ap1.facility.path-cc.io access point.","title":"Account Approval by a Research Computing Facilitator"},{"location":"overview/account_setup/registration/#login","text":"Once your account has been added to a PATh access point, you will be able to log in using a terminal or SSH program. Logging in requires authenticating your credientials using one of two options: web authentication or SSH key pair authentication . Additional information on this process will be provided during and/or following your meeting with a Research Computing Facilitator.","title":"Login"},{"location":"overview/account_setup/registration/#option-1-login-via-web-authentication","text":"Logging in via web authentication requires no preparatory steps beyond having access to an internet browser. To authenticate using this approach: Open a terminal and type ssh username@ap1.facility.path-cc.io , being sure to replace username with your PATh access point username. Upon hitting enter, the following text should appear with a unique, but similar, URL: Authenticate at ----------------- https://cilogon.org/device/?user_code=FF4-ZX6-9LK ----------------- Type 'Enter' when you authenticate. Copy the https:// link, paste it into a web browser, and hit enter. You will be redirected to a new page where you will be prompted to login using your institutional credentials. Once you have done so, a new page will appear with the following text: \"You have successfully approved the user code. Please return to your device for further instructions.\" Return to your terminal, and type 'Enter' to complete the login process.","title":"Option 1: Login via Web Authentication"},{"location":"overview/account_setup/registration/#option-2-login-via-ssh-key-pair-authentication","text":"It is also possible to authenticate using an SSH key pair, if you prefer. Logging in using SSH keys does not require access to an internet browser to login into the PATh access point, ap1.facility.path-cc.io . Full details across multiple configurations are explained in our SSH Authentication guide .","title":"Option 2: Login via SSH Key Pair Authentication"},{"location":"overview/account_setup/registration/#get-help","text":"For questions regarding logging in or creating an account, contact us at support@path-cc.io .","title":"Get Help"},{"location":"overview/account_setup/request-credits/","text":"Request PATh Facility Credits \u00b6 All compute time on the PATh Facility is charged using a credit system. Credits are granted from the NSF and associated with the active project. This guide details the process and templates available to request credits. PATh staff can provide a small startup allocation at any time for initial testing before a full credit request. Requirements \u00b6 The opportunity to use PATh Facility resources is available to all PIs submitting, or having submitted, proposals to the programs listed below: Program Officers How to Apply \u00b6 New Proposals: As documented in the PAPPG 24-1, Ch 2, section E.7 , projects can request credits as part of the award process. Include the request for HTC resources as a Supplementary Document attached to the proposal (as described in the original DCL NSF 22-051). Existing Awards: Email the NSF cognizant program officer of their project, and copy credit-accounts@path-cc.io, with the description of their HTC resource request. Please include \"HTCAccess\" and the award number for the funded project in the e-mail subject line. Sample Credit Request \u00b6 The credit request should include: (1) the anticipated total HTC resources required, with yearly breakdown; and (2) a technical description and justification for the request. The latter should include information regarding (a) the expected number of self-contained tasks per ensemble \u2012 note that each task can be packaged into one or more batch job(s); (b) the resource requirements for each task type in the ensemble \u2012 for example, requirements for cores, memory, wall-time, and scratch space; (c) the expected number of ensembles; (d) the expected input and output data requirements for each task type; and (e) the expected number and size of shared input files within an ensemble \u2013 expected number of times each file is read per ensemble. This should be no more than two pages. The Research Facilitation team has assembled a template credit request for you to use as a starting point: Template for NSF PATh Credit Requests How Many Credits? \u00b6 If you are not sure how many credits to request for your allocation, please do one or more of the following: Use the credit calculator developed by the Facilitation team: Go to the sample credit calculator here: PATh Facility Credit Calculator Follow the instructions in the calculator - make a copy and fill in information about your jobs and ensembles to generate a credit estimate. Contact the PATh Facility team at credit-accounts@path-cc.io Request an account and startup allocation on the PATh Facility to run test jobs and determine resource needs and possible scale: Getting Started The current credit system is documented on this page: PATh Facility Credit Charges . Program Officers \u00b6 Requests can be submitted to the following program officers, based on domain: Computational and Data-Enabled Science and Engineering (CDS&E), NSF, PO: Christina Payne, cpayne@nsf.gov Cyberinfrastructure for Sustained Scientific Innovation (CSSI), CISE/OAC, PO: Tevfik Kosar, tkosar@nsf.gov Innovation: Bioinformatics , BIO/DBI, PO: Jean X. Gao, jgao@nsf.gov Neural Systems , BIO/IOS, PO: Evan Balaban, ebalaban@nsf.gov Collaborative Research in Computational Neuroscience (CRCNS), CISE/IIS, PO: Kenneth Whang, kwhang@nsf.gov \u2012 see also NSF DCL 22-022 Astronomy and Astrophysics Research Grants (AAG), MPS/AST, PO: Andreas Berlind, aberlind@nsf.gov Chemical Theory, Models, and Computational Methods (CTMC), MPS/CHE, PO: Richard Dawes, rdawes@nsf.gov Condensed Matter and Materials Theory (CMMT), MPS/DMR, PO: Daryl Hess, dhess@nsf.gov Atomic, Molecular and Optical Physics \u2012 Theory , MPS/PHY, PO: Robert Forrey, rforrey@nsf.gov Nuclear Physics \u2012 Theory , MPS/PHY, PO: Bogdan Mihaila, MPS/PHY, bmihaila@nsf.gov Geoinformatics (GI), GEO/EAR, PO: Raleigh Martin, ramartin@nsf.gov Geophysics (PH) , GEO/EAR, PO: Eva Zanzerkia, ezanzerk@nsf.gov Arctic Research Opportunities , GEO/OPP, POs: Marc Stieglitz, mstiegli@nsf.gov , Allen Pope, apope@nsf.gov Antarctic Research , GEO/OPP, PO: Allen Pope, apope@nsf.gov","title":"Request PATh Facility Credits"},{"location":"overview/account_setup/request-credits/#request-path-facility-credits","text":"All compute time on the PATh Facility is charged using a credit system. Credits are granted from the NSF and associated with the active project. This guide details the process and templates available to request credits. PATh staff can provide a small startup allocation at any time for initial testing before a full credit request.","title":"Request PATh Facility Credits"},{"location":"overview/account_setup/request-credits/#requirements","text":"The opportunity to use PATh Facility resources is available to all PIs submitting, or having submitted, proposals to the programs listed below: Program Officers","title":"Requirements"},{"location":"overview/account_setup/request-credits/#how-to-apply","text":"New Proposals: As documented in the PAPPG 24-1, Ch 2, section E.7 , projects can request credits as part of the award process. Include the request for HTC resources as a Supplementary Document attached to the proposal (as described in the original DCL NSF 22-051). Existing Awards: Email the NSF cognizant program officer of their project, and copy credit-accounts@path-cc.io, with the description of their HTC resource request. Please include \"HTCAccess\" and the award number for the funded project in the e-mail subject line.","title":"How to Apply"},{"location":"overview/account_setup/request-credits/#sample-credit-request","text":"The credit request should include: (1) the anticipated total HTC resources required, with yearly breakdown; and (2) a technical description and justification for the request. The latter should include information regarding (a) the expected number of self-contained tasks per ensemble \u2012 note that each task can be packaged into one or more batch job(s); (b) the resource requirements for each task type in the ensemble \u2012 for example, requirements for cores, memory, wall-time, and scratch space; (c) the expected number of ensembles; (d) the expected input and output data requirements for each task type; and (e) the expected number and size of shared input files within an ensemble \u2013 expected number of times each file is read per ensemble. This should be no more than two pages. The Research Facilitation team has assembled a template credit request for you to use as a starting point: Template for NSF PATh Credit Requests","title":"Sample Credit Request"},{"location":"overview/account_setup/request-credits/#how-many-credits","text":"If you are not sure how many credits to request for your allocation, please do one or more of the following: Use the credit calculator developed by the Facilitation team: Go to the sample credit calculator here: PATh Facility Credit Calculator Follow the instructions in the calculator - make a copy and fill in information about your jobs and ensembles to generate a credit estimate. Contact the PATh Facility team at credit-accounts@path-cc.io Request an account and startup allocation on the PATh Facility to run test jobs and determine resource needs and possible scale: Getting Started The current credit system is documented on this page: PATh Facility Credit Charges .","title":"How Many Credits?"},{"location":"overview/account_setup/request-credits/#program-officers","text":"Requests can be submitted to the following program officers, based on domain: Computational and Data-Enabled Science and Engineering (CDS&E), NSF, PO: Christina Payne, cpayne@nsf.gov Cyberinfrastructure for Sustained Scientific Innovation (CSSI), CISE/OAC, PO: Tevfik Kosar, tkosar@nsf.gov Innovation: Bioinformatics , BIO/DBI, PO: Jean X. Gao, jgao@nsf.gov Neural Systems , BIO/IOS, PO: Evan Balaban, ebalaban@nsf.gov Collaborative Research in Computational Neuroscience (CRCNS), CISE/IIS, PO: Kenneth Whang, kwhang@nsf.gov \u2012 see also NSF DCL 22-022 Astronomy and Astrophysics Research Grants (AAG), MPS/AST, PO: Andreas Berlind, aberlind@nsf.gov Chemical Theory, Models, and Computational Methods (CTMC), MPS/CHE, PO: Richard Dawes, rdawes@nsf.gov Condensed Matter and Materials Theory (CMMT), MPS/DMR, PO: Daryl Hess, dhess@nsf.gov Atomic, Molecular and Optical Physics \u2012 Theory , MPS/PHY, PO: Robert Forrey, rforrey@nsf.gov Nuclear Physics \u2012 Theory , MPS/PHY, PO: Bogdan Mihaila, MPS/PHY, bmihaila@nsf.gov Geoinformatics (GI), GEO/EAR, PO: Raleigh Martin, ramartin@nsf.gov Geophysics (PH) , GEO/EAR, PO: Eva Zanzerkia, ezanzerk@nsf.gov Arctic Research Opportunities , GEO/OPP, POs: Marc Stieglitz, mstiegli@nsf.gov , Allen Pope, apope@nsf.gov Antarctic Research , GEO/OPP, PO: Allen Pope, apope@nsf.gov","title":"Program Officers"},{"location":"overview/references/PATh_vs_OSPool/","text":"Computing on the PATh Facility vs. OSG's Open Science Pool \u00b6 Getting started with High Throughput Computing (HTC) compute resources is easy and our Research Computing Facilitation Team is here to help you every step along the way. There are two HTC systems funded by the Partnership to Advance High Throughput Computing (PATh) grant that are available free-of-charge to United States open science projects: the PATh Facility and OSG\u2019s Open Science Pool . Both are large-scale distributed High Throughput Computing systems that provide free compute resources . A researcher\u2019s experience on PATh Facility and OSG\u2019s Open Science Pool (OSPool) compute systems is similar: both offer thousands of CPU resources, as well as GPUs, disk space for saving actively-used data, and support technologies such as containerized software and checkpointing. They also both use a HTCondor Software Suite as a job scheduling software, which specializes in managing large high-throughput workflows. There are a few key differences that might impact which system you are interested in: OSG\u2019s OSPool is open to any US-affiliated academic, non-profit, or government research project. OSG\u2019s OSPool does not use a credit system and is optimized for running many small jobs simultaneously. Once your account is activated, you can begin submitting jobs to HTCondor to run on the OSPool. The OSPool is composed of donated compute capacity from institutions from around the United States, meaning individual researchers can run thousands of backfill jobs on this system every day. To get the most out of the OSPool, we recommend jobs submitted to the OSPool have laptop-sized resource profiles. The PATh Facility is a dedicated High Throughput Computing system for researchers funded by the National Science Foundation (NSF). The PATh Facility uses a \u201ccompute credit\u201d system which allows researchers to request larger resource requests for their jobs than can be supported on the OSPool. Researchers can request larger amounts of CPU/GPUs, more memory, disk space, and a longer runtime and are guaranteed these resources until their job completes. Researchers on the PATh Facility start with a startup allocation of credits and should use these startup credits to inform a formal NSF credit request as part of a new or existing grant request. This credit-based system is optimized for running many small, medium, and large HTC jobs simultaneously.","title":"PATh Facility vs. OSG's Open Science Pool"},{"location":"overview/references/PATh_vs_OSPool/#computing-on-the-path-facility-vs-osgs-open-science-pool","text":"Getting started with High Throughput Computing (HTC) compute resources is easy and our Research Computing Facilitation Team is here to help you every step along the way. There are two HTC systems funded by the Partnership to Advance High Throughput Computing (PATh) grant that are available free-of-charge to United States open science projects: the PATh Facility and OSG\u2019s Open Science Pool . Both are large-scale distributed High Throughput Computing systems that provide free compute resources . A researcher\u2019s experience on PATh Facility and OSG\u2019s Open Science Pool (OSPool) compute systems is similar: both offer thousands of CPU resources, as well as GPUs, disk space for saving actively-used data, and support technologies such as containerized software and checkpointing. They also both use a HTCondor Software Suite as a job scheduling software, which specializes in managing large high-throughput workflows. There are a few key differences that might impact which system you are interested in: OSG\u2019s OSPool is open to any US-affiliated academic, non-profit, or government research project. OSG\u2019s OSPool does not use a credit system and is optimized for running many small jobs simultaneously. Once your account is activated, you can begin submitting jobs to HTCondor to run on the OSPool. The OSPool is composed of donated compute capacity from institutions from around the United States, meaning individual researchers can run thousands of backfill jobs on this system every day. To get the most out of the OSPool, we recommend jobs submitted to the OSPool have laptop-sized resource profiles. The PATh Facility is a dedicated High Throughput Computing system for researchers funded by the National Science Foundation (NSF). The PATh Facility uses a \u201ccompute credit\u201d system which allows researchers to request larger resource requests for their jobs than can be supported on the OSPool. Researchers can request larger amounts of CPU/GPUs, more memory, disk space, and a longer runtime and are guaranteed these resources until their job completes. Researchers on the PATh Facility start with a startup allocation of credits and should use these startup credits to inform a formal NSF credit request as part of a new or existing grant request. This credit-based system is optimized for running many small, medium, and large HTC jobs simultaneously.","title":"Computing on the PATh Facility vs. OSG's Open Science Pool"},{"location":"overview/references/credit-account-charges/","text":"Credit Account Charges \u00b6 This page describes the specific credit charges for certain resources on the PATh Facility . For details on how to request credits for your project, see Request PATh Facility Credits Credits may be requested and used under the CPU Credit category or the GPU Credit category. Credits are specific to each node type \u2013 CPU Credits are not transferable to GPU Credits, and vice-versa. CPU Credit charges are defined by a combination of per-job cores and memory request, while GPU Credit charges are defined by a combination of per-job GPU, CPU, and memory request. CPU Credits \u00b6 Cores per job Credit charge per core hour 1 1.0 2-8 1.2 8-32 1.5 >32 2.0 When running on a resource with hyperthreaded cores, a 40% discount is applied; e.g., 1 hyperthreaded core for 1 hour costs 0.6 credits. Additional CPU Credit Charge \u00b6 Memory Per Core \u00b6 When more than 2GB per core of memory is requested by the job, there\u2019s an additional per-GB charge for memory. Memory (GB) per job Credit charge per hour, per GB Up to 2GB per core (\"nominal\") No charge 2-8 GB greater then nominal 0.125 8-32 0.25 32-128 0.375 128-512 0.50 CPU Credit Example \u00b6 If an 8 core job requests 128 GB of RAM, there is an additional charge for 128 \u2013 (8 * 2) = 112 GB of RAM. If the job ran for an hour, it would use 0.375 * 112 = 42 credits total for memory and 1.2 * 8 = 9.6 credits for CPU. GPU Credits \u00b6 GPUs per job Credit charge per GPU hour 1 1.0 2 1.2 3 1.5 4 2.0 Additional GPU Credit Charge \u00b6 CPUS per GPU \u00b6 When more than 16 cores per GPU is requested by the job, there\u2019s an additional per-core charge for the CPU cores beyond the nominal. CPUs per GPU Credit charge per hour, per core Up to 16 cores per GPU (\"nominal\") No charge 16-48 cores per GPU 0.125 48-64 0.20 Memory per GPU \u00b6 When more than 2 GB per core of memory is requested by the job, there\u2019s an additional per-GB charge for memory for the beyond-nominal memory usage. Memory (GB) per job Credit charge per hour, per GB Up to 128GB per GPU (\"nominal\") No charge 128-384 GB per GPU 0.012 384-512 0.020 GPU Credit Example \u00b6 If a 1 GPU job requests 256 GB of RAM and 32 cores, there is an additional charge for 256 \u2013 (1 * 128) = 128 GB of RAM and an additional charge for 32 \u2013 (1 * 16) = 16 cores. If the job ran for an hour, it would use 0.012 * 128 = 1.536 credits total for memory, 0.125 * 16 = 2 credits for CPU, and 1.0 * 1 = 1.0 credits for GPU.","title":"Credit Account Charges"},{"location":"overview/references/credit-account-charges/#credit-account-charges","text":"This page describes the specific credit charges for certain resources on the PATh Facility . For details on how to request credits for your project, see Request PATh Facility Credits Credits may be requested and used under the CPU Credit category or the GPU Credit category. Credits are specific to each node type \u2013 CPU Credits are not transferable to GPU Credits, and vice-versa. CPU Credit charges are defined by a combination of per-job cores and memory request, while GPU Credit charges are defined by a combination of per-job GPU, CPU, and memory request.","title":"Credit Account Charges"},{"location":"overview/references/credit-account-charges/#cpu-credits","text":"Cores per job Credit charge per core hour 1 1.0 2-8 1.2 8-32 1.5 >32 2.0 When running on a resource with hyperthreaded cores, a 40% discount is applied; e.g., 1 hyperthreaded core for 1 hour costs 0.6 credits.","title":"CPU Credits"},{"location":"overview/references/credit-account-charges/#additional-cpu-credit-charge","text":"","title":"Additional CPU Credit Charge"},{"location":"overview/references/credit-account-charges/#memory-per-core","text":"When more than 2GB per core of memory is requested by the job, there\u2019s an additional per-GB charge for memory. Memory (GB) per job Credit charge per hour, per GB Up to 2GB per core (\"nominal\") No charge 2-8 GB greater then nominal 0.125 8-32 0.25 32-128 0.375 128-512 0.50","title":"Memory Per Core"},{"location":"overview/references/credit-account-charges/#cpu-credit-example","text":"If an 8 core job requests 128 GB of RAM, there is an additional charge for 128 \u2013 (8 * 2) = 112 GB of RAM. If the job ran for an hour, it would use 0.375 * 112 = 42 credits total for memory and 1.2 * 8 = 9.6 credits for CPU.","title":"CPU Credit Example"},{"location":"overview/references/credit-account-charges/#gpu-credits","text":"GPUs per job Credit charge per GPU hour 1 1.0 2 1.2 3 1.5 4 2.0","title":"GPU Credits"},{"location":"overview/references/credit-account-charges/#additional-gpu-credit-charge","text":"","title":"Additional GPU Credit Charge"},{"location":"overview/references/credit-account-charges/#cpus-per-gpu","text":"When more than 16 cores per GPU is requested by the job, there\u2019s an additional per-core charge for the CPU cores beyond the nominal. CPUs per GPU Credit charge per hour, per core Up to 16 cores per GPU (\"nominal\") No charge 16-48 cores per GPU 0.125 48-64 0.20","title":"CPUS per GPU"},{"location":"overview/references/credit-account-charges/#memory-per-gpu","text":"When more than 2 GB per core of memory is requested by the job, there\u2019s an additional per-GB charge for memory for the beyond-nominal memory usage. Memory (GB) per job Credit charge per hour, per GB Up to 128GB per GPU (\"nominal\") No charge 128-384 GB per GPU 0.012 384-512 0.020","title":"Memory per GPU"},{"location":"overview/references/credit-account-charges/#gpu-credit-example","text":"If a 1 GPU job requests 256 GB of RAM and 32 cores, there is an additional charge for 256 \u2013 (1 * 128) = 128 GB of RAM and an additional charge for 32 \u2013 (1 * 16) = 16 cores. If the job ran for an hour, it would use 0.012 * 128 = 1.536 credits total for memory, 0.125 * 16 = 2 credits for CPU, and 1.0 * 1 = 1.0 credits for GPU.","title":"GPU Credit Example"},{"location":"support_and_training/support/getting-help-from-RCFs/","text":"Email, Office Hours, and 1-1 Meetings \u00b6 There are multiple ways to get help from PATh\u2019s Research Computing Facilitators. Get in touch anytime! Research Computing Facilitators \u00b6 To help researchers effectively utilize computing resources, our Research Computing Facilitators (RCFs) not only assist you in implementing your computational work on PATh compute resources, but can also point you to other services related to research computing and data needs. Help via Email \u00b6 We provide ongoing support via email to support@path-cc.io, and it\u2019s never a bad idea to start by sending questions or issues via email. You can typically expect a first response within a few business hours. Virtual Office Hours \u00b6 Drop-in for live help, available every Tuesday and Thursday: Tuesdays, 4-5:30pm ET / 1-2:30pm PT Thursdays, 11:30am-1pm ET / 8:30-10am PT You can find the URL to the Virtual Office Hours meeting room when you log into a PATh access point, or in the signature of a support email from an RCF. Click here to sign-in for office hours, once you arrive in the room. Make an Appointment \u00b6 We are happy to arrange meetings outside of designated Office Hours, per your preference. Simply email us at support@path-cc.io, and we will set up a time to meet! Learn About the PATh Facility \u00b6 The following slides provide an overview about the PATh Facility: why it exists, good use cases, and how to get started: Slides","title":"Email, Office Hours, and Individual Consultations"},{"location":"support_and_training/support/getting-help-from-RCFs/#email-office-hours-and-1-1-meetings","text":"There are multiple ways to get help from PATh\u2019s Research Computing Facilitators. Get in touch anytime!","title":"Email, Office Hours, and 1-1 Meetings"},{"location":"support_and_training/support/getting-help-from-RCFs/#research-computing-facilitators","text":"To help researchers effectively utilize computing resources, our Research Computing Facilitators (RCFs) not only assist you in implementing your computational work on PATh compute resources, but can also point you to other services related to research computing and data needs.","title":"Research Computing Facilitators"},{"location":"support_and_training/support/getting-help-from-RCFs/#help-via-email","text":"We provide ongoing support via email to support@path-cc.io, and it\u2019s never a bad idea to start by sending questions or issues via email. You can typically expect a first response within a few business hours.","title":"Help via Email"},{"location":"support_and_training/support/getting-help-from-RCFs/#virtual-office-hours","text":"Drop-in for live help, available every Tuesday and Thursday: Tuesdays, 4-5:30pm ET / 1-2:30pm PT Thursdays, 11:30am-1pm ET / 8:30-10am PT You can find the URL to the Virtual Office Hours meeting room when you log into a PATh access point, or in the signature of a support email from an RCF. Click here to sign-in for office hours, once you arrive in the room.","title":"Virtual Office Hours"},{"location":"support_and_training/support/getting-help-from-RCFs/#make-an-appointment","text":"We are happy to arrange meetings outside of designated Office Hours, per your preference. Simply email us at support@path-cc.io, and we will set up a time to meet!","title":"Make an Appointment"},{"location":"support_and_training/support/getting-help-from-RCFs/#learn-about-the-path-facility","text":"The following slides provide an overview about the PATh Facility: why it exists, good use cases, and how to get started: Slides","title":"Learn About the PATh Facility"}]}